# Accessing Data {#AccessingData}

> "Data is the new oil." â€”Clive Humby, Chief Data Scientist, Starcount

In this chapter, we'll cover how to access data given in various forms and provided from various sources.

## Rectangular vs. Non-rectangular Data

Data present themselves in many forms, but at a basic level, all data can be categorized into two structures: __rectangular data__ and __non-rectangular data__. Intuitively, rectangular data are shaped like a rectangle where every value corresponds to some row and column. Most data frames store rectangular data. Non-rectangular data, on the other hand, are not neatly arranged in rows and columns. Instead, they are often a culmination of separate data structures where there is some similarity among members of the same data structure. Usually non-rectangular data are stored in lists.

To motivate this idea, let's consider a basic grocery list which consists of ten items: black beans, milk, pasta, cheese, bananas, peanut butter, bread, apples, tomato sauce, and mayonnaise. Notice, there is little organization to this list, and more involved shoppers may find this list inadequate or unhelpful. We may wish to group these items by sections in which we're likely to find them. We may also want to include prices, so we know in-store whether the items are on sale. Let's consider two distinct (but legitimate) ways to organize these data. 

```{block, type = 'caution'}
To illustrate the idea of rectangular vs. non-rectangular data, we will consider how these data can be structured in both ways using `R`. You may not have seen some of these functions yet. No worries! The objective is not to understand __how__ to utilize these functions but to comprehend the difference between rectangular and non-rectangular data.
```

One may first consider grouping these items by section. For example, apples and bananas can be found in the produce section, whereas black beans and tomato sauce can be found in the canned goods section. If we were to continue to group these items by section, we may arrive at a data set which looks something like this:

```{r}
groc <- list(produce = data.frame(item = c('apples', 'bananas'),
                                  price = c(3.99, 0.49)),
             condiments = data.frame(item = c('peanut_butter', 'mayonnaise'),
                                     price = c(2.18, 3.89)),
             canned_goods = data.frame(item = c('black_beans', 'tomato_sauce'),
                                       price = c(0.99, 0.69)),
             grains = data.frame(item = c('bread', 'pasta'),
                                 price = c(2.99, 1.99)),
             dairy = data.frame(item = c('milk', 'butter'),
                                price = c(2.73, 2.57)))
groc
```

Here, we use lists and data frames to create a data set of our grocery list. This list can be traversed depending on what section of the store we find ourselves in. For example, suppose we are in the produce section, and we need to recall what items to buy. We could utilize the following code to remind ourselves.

```{r}
groc$produce
```

```{block, type = 'reflect'}
Is this grocery list an example of rectangular or non-rectangular data (try to use the definition of rectangular data given above)? Are there examples of rectangular data contained within the grocery list? How could we restructure the data to __rectangularize__ the grocery list?
```

As constructed, this grocery list is an example of non-rectangular data. As a whole, the grocery list is not shaped like a rectangle, but rather, consists of sets of rectangular data, where the sets are defined by the section of the store. Within a section of the store, the items and prices are given in rectangular form since every value is defined by a row and column.

While lists are often a useful return object for user-defined functions, they are often troublesome to work with because they may be non-rectangular. If a data set can be restructured or created in rectangular form, it should be. Rectangular data is especially important within the Tidyverse, a self-described 'opinionated collection of R packages designed for data science'. All packages within the Tidyverse rely on the principle of _tidy data_, data structures where observations are given by rows and variables are given by columns. As defined, tidy data are rectangular, so as we embark on wrangling, visualizing, and modeling data in future chapters, it is important to ponder the nature of our data and whether it can be rectangularized.  

```{block, type='progress'}
Create your own example of non-rectangular data. Make sure your example is assigned to a variable with an appropriate name.
```

Let's consider how we can rectangularize the grocery list. Instead of creating a list of named data frames, where the name represents the section of the store, let's create a grocery list where each row represents an item and columns specify the section and price. Because the Tidyverse requires rectangular data, there are several functions which are handy for converting data structures to rectangular form. We could utilize one of these functions to rectangularize the data set. 

```{r}
library(tidyverse, quietly = TRUE)
groc_rec <- groc %>%
  bind_rows(., .id = 'section')
groc_rec
```

```{block, type='bonus'}
The code chuck above is an example of "piping", a concept used heavily in the `tidyverse` which allows the easy chaining of actions, where the output of one function is "piped" to the input argument of the next function. Here the `groc` list is being piped into the first argument (denoted with a `.`) of the `bind_rows` function. Though it may look confusing at first, piping is a nice way to increase code readability, since the order that functions will be applied matches the order they appear in the code (from top to bottom). For more information, see [here](https://r4ds.had.co.nz/pipes.html).
```


Or, we can simply create the grocery list in rectangular form to begin with.


```{block, type='progress'}
Create a data frame called `groc_df` which contains the above grocery list in rectangular form.
```


```{block, type='feedback'}
Any feedback for this section? Click [here](https://docs.google.com/forms/d/e/1FAIpQLSePQZ3lIaCIPo9J2owXImHZ_9wBEgTo21A0s-A1ty28u4yfvw/viewform?entry.1684471501=The%20R%20Community)
```

### Reading and Writing Rectangular Data

Rectangular data are often stored locally using text files (.txt), comma separated value files (.csv), and Excel files (.xlsx). When data are written to these file types, they are easy to view across devices, without the need for `R`. Since most grocery store trips obviate the need for `R`, let's consider how to write our grocery list to each of these file types. To write to and read from data text files or comma separated value files, the `readr` package will come in handy, whereas the `xlsx` package will allow us to write to and read from Excel files. To write data from `R` to a file, we will leverage commands beginning with `write`.

```{r, eval=FALSE}
# text file
readr::write_delim(groc_rec, path = 'data_raw/groceries-rectangular.txt')
# csv file
readr::write_csv(groc_rec, path = 'data_raw/groceries-rectangular.csv')
# Excel file
xlsx::write.xlsx(groc_rec, file = 'data_raw/groceries-rectangular.xlsx', row.names = FALSE)  # don't include the row names
```


To read data from a file to `R`, we will leverage commands beginning with `read`. Before reading data into `R`, you will need to look at the file and file extension to better understand which function to use.

```{r, eval=FALSE}
# text file
readr::read_delim('data_raw/groceries-rectangular.txt', delim = ' ', lazy = FALSE)  # the file delimits columns using a space
# csv file
readr::read_csv('data_raw/groceries-rectangular.csv', lazy = FALSE)
# Excel file
xlsx::read.xlsx('data_raw/groceries-rectangular.xlsx', sheetName = 'Sheet1')  # load the sheet called 'Sheet1'
```

```{block, type = 'caution'}
Reading files into `R` can sometimes be frustrating. Always look at the data to see if there are column headers and row names. Text files can have different __delimiters__, characters which separate values in a data set. The default delimiter for `readr::write_delim()` is a space, but other common text delimiters are tabs, colons, semi-colons, or vertical bars. Commas are so commonly used as a delimiter, it gets a function of its own (.csv). Always ensure that data from an Excel spreadsheet are rectangular. Lastly, the `readr` package will guess the data type of each column. Check these data types are correct using `str()`.
```

```{block, type='bonus'}
Reading and writing csv and text files can also be done with the `read.csv`, `write.csv`, `read.table` and `write.table` functions included in base R. However, the `readr` package belongs to the tidyverse and has additional capabilities (see [here]{https://readr.tidyverse.org} for more info).
```

```{block, type='progress'}
Write the `groc_df` data frame that you created above to your computer using the `write_delim` function and using the `write_csv` function, using different file names. Open the files using a text editor (not R) and comment on the differences in formatting between the between them.
```

```{block, type='feedback'}
Any feedback for this section? Click [here](https://docs.google.com/forms/d/e/1FAIpQLSePQZ3lIaCIPo9J2owXImHZ_9wBEgTo21A0s-A1ty28u4yfvw/viewform?entry.1684471501=The%20R%20Community)
```

### Reading and Writing Non-rectangular Data

Writing non-rectangular data from R to your local machine is easy with the help of `write_rds()` from the `readr` package. While the origin of 'RDS' is unclear, some believe it stands for R data serialization. Nonetheless, RDS files store single R objects, regardless of the structure. This means that RDS files are a great choice for data which cannot be written to rectangular file formats such as text, csv, and Excel files.

The sister function entitled `read_rds()` allows you to read any RDS file directly into your current `R` environment, assuming the file already exists.

```{block, type = 'bonus'}
Similar to RDS files, there are also RData files which can store multiple `R` objects. These files can be written from `R` to your local machine using `save()` and read from your local machine to R using `load()`. We recommend avoiding RData files, and instead, storing multiple `R` objects in one named list which is then saved as an RDS file.
```

When there is inevitably non-rectangular data that exist which you would like to load into `R`, you are in for a treat. The rest of this module can loosely be viewed as a guide to managing and curating data. We will leverage many tools to tackle this problem, but in the next two sections, we will address two specific, common instances of non-rectangular data: data from APIs and from scraped sources.

```{block, type='progress'}
Write the non-rectangular data that you created above to your computer using the `write_rds()` function. Remember that you can use `?write_rds()` to learn more about the function.
```

```{block, type='feedback'}
Any feedback for this section? Click [here](https://docs.google.com/forms/d/e/1FAIpQLSePQZ3lIaCIPo9J2owXImHZ_9wBEgTo21A0s-A1ty28u4yfvw/viewform?entry.1684471501=The%20R%20Community)
```

## APIs: Clean and Curated {#APIs}

An application programming interface (API) is a set of functions and procedures which allows one computer program to interact with another. To simplify the concept remarkably, we will consider web-APIs where there is a server (computer waiting to provide data) and a client (computer making a request for data).

The benefit of APIs is the result: clean and curated data from the host. The pre-processing needed to get the data in a workable form is entirely done on the server side. We, however, are responsible for making the request. Web-APIs often utilize JavaScript Object Notation (JSON), another example of non-rectangular data. We will utilize the `httr` and the `jsonlite` packages to retrieve the latest sports lines from Bovada, an online sportsbook.

Before we start, we'll need to download the `httr` and `jsonlite` packages and load them into our current environment. Furthermore, we will need to find the address of the server to which we will send the request.

```{r}
library(httr, quietly = TRUE)
library(jsonlite, quietly = TRUE)
bov_nfl_api <- "https://www.bovada.lv/services/sports/event/v2/events/A/description/football/nfl"
```

To ask for data through a web-API, we will need to make a `GET` request with the `httr` package's `GET()` function. After making the request, we can read about the server's response.

```{r, cache=TRUE}
bov_req <- httr::GET(url = bov_nfl_api)
bov_req
```

If the request was successful, then the status of the request will read 200. Otherwise, there was some error with your request. For a list of HTTP status codes and their respective definitions, follow this [link](https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html). Since the response clarifies that the content is indeed driven by JavaScript, then we will utilize the `jsonlite` package to read the JSON structured data. A handy function we will use will be `fromJSON()` which converts a character vector containing data in JSON structure to native structures in `R` like lists. So, in order, we will

1. Extract the content from the server's response
1. Convert the content to a character vector, maintaining the JSON structure
1. Restructure the data into native `R` structures, using `fromJSON()`.


```{r}
content <- bov_req$content
content_char <- rawToChar(content)
bov_res <- jsonlite::fromJSON(content_char)
```

Of course, we could also create a function which takes the server's response and converts the content to native `R` structures. We will want to code in a force stop if the response status is not 200. We will also want to require the `httr` and `jsonlite` packages which will automatically install the packages if a user calls the function without having the packages installed.

```{r}
convert_JSON <- function(resp){
  # call needed packages
  require(httr)
  require(jsonlite)
  # stop if the server returned an error
  httr::stop_for_status(resp)
  # return JSON content in native R structures
  return(jsonlite::fromJSON(rawToChar(resp$content)))
}
```

Finally, we can get the same output by simply calling the function.

```{r}
identical(convert_JSON(bov_req), bov_res)
```

```{block, type = 'bonus'}
Some web-APIs require additional information from the us as outlined in the documentation for the API. In this case, the user would need to provide additional query parameters in their GET request. Thankfully, this functionality is ingrained in the `httr` package's `GET()` function. For more information on how to include query parameters, type `??GET` into your `R` console.
```

```{block, type='feedback'}
Any feedback for this section? Click [here](https://docs.google.com/forms/d/e/1FAIpQLSePQZ3lIaCIPo9J2owXImHZ_9wBEgTo21A0s-A1ty28u4yfvw/viewform?entry.1684471501=The%20R%20Community)
```

## Scraping: Messy and Mangled

If you are reading this textbook, at some point in your career, you are likely to want or need data which exists on the web. You have looked for downloadable sources and Google searched for an API, but alas, no luck. The last resort for importing data into `R` is __web scraping__. Web scraping is a technique for harvesting data which is portrayed on the web and exists in hypertext markup language (HTML), the language of web browser documents. 

### Scraping vs APIs

The benefit of using an API are clean data. For example, we can traverse the result to find the latest NFL events.

```{r}
head(bov_res[[2]][[1]][,2])
```

With more digging, we can find which teams are playing at home.

```{r}
head(bov_res[[2]][[1]][[16]])
```

We can also find the current line of each of these games. Here, I have created a function called `get_bovada_lines()` which traverses this complicated (yet clean) JSON object using methods explored in Chapter 3 and combines the information together into a rectangular data set.

```{r}
bov_res %>%
  get_bovada_lines()
```

While traversing these sometimes complicated lists may seem intimidating, with practice, working with data from an API will be made easier after discussing mapping functions in Chapter 3 which are useful for traversing complicated lists. Hopefully, after the scraping section, you will find working with APIs like a walk in the park compared to scraping data directly from the web.

### Lessons Learned from Scraping {#LessonsLearnedFromScraping}

Scraping is a necessary evil that requires patience. While some tasks may prove easy, you will quickly find others seem insurmountable. In this section, we will outline a few tips to help you become a web scraper.

1. __Brainstorm__! Before jumping into your scraping project, ask yourself _what data do I need_ and _where can I find it_? If you discover you need data from various sources, _what is the unique identifier_, the link which ties these data together? Taking the time to explore different websites can save you a vast amount of time in the long run. As a general rule, simplistic looking websites are generally easier to scrape and often contain the same information as more complicated websites with several bells and whistles.

2. __Start small__! Sometimes a scraping task can feel daunting, but it is important to _view your project as a war, splitting it up into small battles_. If you are interested in the racial demographics of each of the United States, consider how you can first scrape this information for one state. In this process, don't forget tip 1! 

3. __Hyperlinks are your friend__! They can lead to websites with more detailed information or serve as the unique identifier you need between different data sources. Sometimes you won't even need to scrape the hyperlinks to navigate between webpages, making minor adjustments to the web address will sometimes do.

4. __Data is everywhere__! Text color, font, or highlighting may serve as valuable data that you need. If these features exist on the webpage, then they exist within the HTML code which generated the document. Sometimes these features are well hidden or even inaccessible, leading to the last and final tip.

5. __Ready your search engine__! Just like coding in `R` is an art, web developing is an art. When asking distinct developers to create the same website with the same functionality, the final result may be similar but the underlying HTML code could be drastically different. Why does this matter? You will run into an issue that hasn't been addressed in this text. Thankfully, if you've run into an issue, someone else probably has too. We cannot recommend websites like [Stack Overflow](https://stackoverflow.com/) enough. 

### Tools for Scraping

Before we can scrape information from a webpage, we need a bit of background on how this information is stored and presented. The goal of this subsection is to briefly introduce the languange of the web, hypertext markup language (HTML). When we talk about scraping the web, what we really mean is gathering bits of information from the HTML code used to build a webpage. Like `R` code, HTML can be overwhelming. The goal is not to teach HTML but to introduce its components, so you have a much more intuitive sense of what we are doing when we scrape the web.

#### Hypertext Markup Language (HTML)

Web sites are written in hypertext markup language. All contents that are displayed on a web page are structured through HTML with the help of HTML __elements__. HTML elements consist of a tag and contents. The __tag__ defines how the web browser should format and display the content. Aptly, the __content__ is what should be displayed. 

For example, if we wished to format text as a paragraph within the web document, then we could use the paragraph tag, `<p>`, to indicate the beginning of a paragraph. After opening a tag, we then specify the content to display before closing the tag. A complete paragraph may read:

`<p> This is the paragraph you want to scrape. </p>`

__Attributes__ are optional parameters which provide additional information about the element in which the attribute is included. For example, within the paragraph tag, you can define a class attribute which formats the text in a specific way, such as bolding, coloring, or aligning the text. To extend our example, the element may read:

`<p class = "fancy"> This is the paragraph you want to scrape which has been formatted in a fancy script. </p>`

The type of attribute, being class, is the attribute __name__, whereas the quantity assigned to the attribute, being fancy, is the attribute __value__. The general decomposition of an HTML element is characterized by the following figure:

```{r, fig.cap="the lingo of an HTML element", echo=F}
knitr::include_graphics("src/images/element_decomp.jpg")
```

```{block, type='bonus'}
The class attribute is a flexible one. Many web developers use the class attribute to point to a class name in a style sheet or to access and manipulate elements with the specific class name with a JavaScript. For more information of the class attribute, see this [link](https://www.w3schools.com/html/html_classes.asp). For more information on cascading style sheets which are used to decorate HTML pages, see this [link](https://www.w3schools.com/css/).
```

```{block, type='feedback'}
Any feedback for this section? Click [here](https://docs.google.com/forms/d/e/1FAIpQLSePQZ3lIaCIPo9J2owXImHZ_9wBEgTo21A0s-A1ty28u4yfvw/viewform?entry.1684471501=The%20R%20Community)
```

#### Selector Gadgets

While all web pages are composed of HTML elements, the elements themselves can be structured in complicated ways. Elements are often nested inside one another or make use of elements in other documents. These complicated structures can make scraping data difficult. Thankfully, we can circumvent exploring these complicated structures with the help of selector gadgets.

A __selector gadget__ allows you to determine what css selector you need to extract the information desired from a webpage. These JavaScript bookmarklets allow you to determine where the information you desire belongs within the complicated structure of elements that makeup a webpage. To follow along in Chapter 3, you will need to download one of these gadgets from this [link](https://selectorgadget.com/). If you use Google Chrome, you can download the bookmark extension directly from this [link](https://selectorgadget.com/).

If the selector gadget fails us, we can always view the structure of the elements directly by viewing the page source. This can be done by right-clicking on the webpage and selecting 'View Page Source'. For Google Chrome, you can also use the keyboard shortcut 'CTRL-U'.

### Scraping NFL Data

In Chapter \@ref(APIs), we gathered some betting data pertaining to the NFL through a web-API. We may wish to supplement these betting data with data pertaining to NFL teams, players, or even playing conditions. The goal in this subsection is to introduce you to scraping by heeding the advice given in the Chapter \@ref(LessonsLearnedFromScraping). Further examples are given in the supplemental material.

Following our own advice, let's brainstorm. When you think of NFL data, you probably think of [NFL.com](https://www.nfl.com/stats) or [ESPN](https://www.espn.com/nfl/stats). These sites obviously have reliable data, but the webpages are pretty involved. While the filters, dropdown menus, and graphics lend great experiences for web browsers, they create headaches for web scrapers. After further digging, we will explore [Pro Football Reference](https://www.pro-football-reference.com/), a reliable archive for football statistics (with a reasonably simple webpage). This is an exhaustive source which boasts team statistics, player statistics, and playing conditions for various seasons. Let's now start small by focusing on team statistics, but further, let's limit our scope to the [2020 Denver Broncos](https://www.pro-football-reference.com/teams/den/2020.htm). Notice, there are hyperlinks for each [player](https://www.pro-football-reference.com/players/G/GordMe00.htm) documented in any of the categories, as well hyperlinks for each game's [boxscore](https://www.pro-football-reference.com/boxscores/202009140den.htm) where there is information about playing conditions and outcomes. Hence, we have a common thread between team statistics, players, and boxscores. If, for example, we chose to scrape team statistics from one website and player statistics from another website, we may have to worry about a unique identifier (being team) if the websites have different naming conventions.

#### HTML Tables: Team Statistics

We'll start with the team statistics for the 2020 Denver Broncos which can be found in a table entitled 'Team Stats and Rankings'. We'll need to figure in which element or __node__ the table lives within the underlying HTML. To do this, we will utilize the CSS selector gadget. If we highlight over and click the table with the selector gadget, we will see that the desired table lives in an element called '#team_stats'.

```{r, fig.cap="finding the team statistics element using the selector gadget", echo=F}
knitr::include_graphics("src/images/broncos_selector_gadget.png")
```

Alternatively, we could view the page source and search for the table name. I've highlighted the information identified by the selector gadget with the cursor.

```{r, fig.cap="finding the team statistics element using the page source", echo=F}
knitr::include_graphics("src/images/broncos_page_source.png")
```

```{block, type = 'caution'}
While the selector gadget is always a great first option, it is not always reliable. There are instances when the selector gadget identifies a node that is hidden or inaccessible without JavaScript. In these situations, it is best view the page source directly for more guidance on how to proceed. Practice with both the selector gadget and the page source.
```

Once we have found the name of the element containing the desired data, we can utilize the `rvest` package to scrape the table. The general process for scraping an HTML table is

1. Read the HTML identified by the web address.
2. Isolate the node containing the data we desire.
3. Parse the HTML table.
4. Take a look at the data to ensure the columns are appropriate labels.

```{r}
library(rvest)
library(janitor)

pfr_url <- "https://www.pro-football-reference.com"
broncos_url <- str_c(pfr_url, '/teams/den/2020.htm')

broncos_url %>%
  # read the HTML
  read_html(.) %>%
  # isolate the node containing the HTML table
  html_node(., css = '#team_conversions') %>%
  # parse the html table
  html_table(.) %>%
  # make the first row of the table column headers and clean up column names
  row_to_names(., row_number = 1) %>%
  clean_names()
```

While these data need cleaning up before they can be used in practice, we will defer these responsibilities to Chapter \@ref(Wrangling).

```{block, type = 'progress'}
Take this time to scrape the 'Team Conversions' table on your own.
```

```{block, type = 'caution'}
While it is exciting to scrape your first nuggets of data, we have just scratched the surface of web scraping. To be honest, it took some time to find data this easy to scrape. More often than not, difficulties arise. The HTML table you want may be commented out or hidden. Your data may not be in the form of a table, at all. For a more in-depth exposition of web scraping, see the supplemental materials.
```

```{block, type='feedback'}
Any feedback for this section? Click [here](https://docs.google.com/forms/d/e/1FAIpQLSePQZ3lIaCIPo9J2owXImHZ_9wBEgTo21A0s-A1ty28u4yfvw/viewform?entry.1684471501=The%20R%20Community)
``` 