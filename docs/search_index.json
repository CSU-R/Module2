[["index.html", "R Module 2 Chapter 1 Welcome!", " R Module 2 Connor Gibbs1 12 Nov, 2021, 03:20 PM Chapter 1 Welcome! Hi, and welcome to the R Module 2 (AKA STAT 158) course at Colorado State University! This course is the second of three 1 credit courses intended to introduce the R programming language, specifically the Tidyverse. Through these Modules (courses), well explore how R can be used to do the following: Access data via files or web application programming interfaces (APIs) Scrape data from web Wrangle and clean complicated data structures Create graphics with an eye for quality and aesthetics Understand data using basic modeling In addition, youll also be exposed to broader concepts, including: Data organization and storage Hypertext Markup Language (HTML) Tidyverse principles More detail will be provided in the Course Topics laid out in the next chapter. 1.0.1 How To Navigate This Book To move quickly to different portions of the book, click on the appropriate chapter or section in the the table of contents on the left. The buttons at the top of the page allow you to show/hide the table of contents, search the book, change font settings, download a pdf or ebook copy of this book, or get hints on various sections of the book. The faint left and right arrows at the sides of each page (or bottom of the page if its narrow enough) allow you to step to the next/previous section. Heres what they look like: Figure 1.1: Left and right navigation arrows Department of Statistics, Colorado State University, connor.gibbs@colostate.edu "],["associated-csu-course.html", "1.1 Associated CSU Course", " 1.1 Associated CSU Course This bookdown book is intended to accompany the associated course at Colorado State University, but the curriculum is free for anyone to access and use. If youre reading the PDF or EPUB version of this book, you can find the live version at https://csu-r.github.io/Module2/, and all of the source files for this book can be found at https://github.com/CSU-R/Module2. If youre not taking the CSU course, you will periodically encounter instructions and references which are not relevant to you. For example, we will make reference to the Canvas website, which only CSU students enrolled in the course have access to. "],["AccessingData.html", "Chapter 2 Accessing Data", " Chapter 2 Accessing Data Data is the new oil. Clive Humby, Chief Data Scientist, Starcount In this chapter, well cover how to access data given in various forms and provided from various sources. "],["rectangular-vs.-non-rectangular-data.html", "2.1 Rectangular vs. Non-rectangular Data", " 2.1 Rectangular vs. Non-rectangular Data Data present themselves in many forms, but at a basic level, all data can be categorized into two structures: rectangular data and non-rectangular data. Intuitively, rectangular data are shaped like a rectangle where every value corresponds to some row and column. Most data frames store rectangular data. Non-rectangular data, on the other hand, are not neatly arranged in rows and columns. Instead, they are often a culmination of separate data structures where there is some similarity among members of the same data structure. Usually non-rectangular data are stored in lists. To motivate this idea, lets consider a basic grocery list which consists of ten items: black beans, milk, pasta, cheese, bananas, peanut butter, bread, apples, tomato sauce, and mayonnaise. Notice, there is little organization to this list, and more involved shoppers may find this list inadequate or unhelpful. We may wish to group these items by sections in which were likely to find them. We may also want to include prices, so we know in-store whether the items are on sale. Lets consider two distinct (but legitimate) ways to organize these data. To illustrate the idea of rectangular vs. non-rectangular data, we will consider how these data can be structured in both ways using R. You may not have seen some of these functions yet. No worries! The objective is not to understand how to utilize these functions but to comprehend the difference between rectangular and non-rectangular data. One may first consider grouping these items by section. For example, apples and bananas can be found in the produce section, whereas black beans and tomato sauce can be found in the canned goods section. If we were to continue to group these items by section, we may arrive at a data set which looks something like this: groc &lt;- list(produce = data.frame(item = c(&#39;apples&#39;, &#39;bananas&#39;), price = c(3.99, 0.49)), condiments = data.frame(item = c(&#39;peanut_butter&#39;, &#39;mayonnaise&#39;), price = c(2.18, 3.89)), canned_goods = data.frame(item = c(&#39;black_beans&#39;, &#39;tomato_sauce&#39;), price = c(0.99, 0.69)), grains = data.frame(item = c(&#39;bread&#39;, &#39;pasta&#39;), price = c(2.99, 1.99)), dairy = data.frame(item = c(&#39;milk&#39;, &#39;butter&#39;), price = c(2.73, 2.57))) groc $produce item price 1 apples 3.99 2 bananas 0.49 $condiments item price 1 peanut_butter 2.18 2 mayonnaise 3.89 $canned_goods item price 1 black_beans 0.99 2 tomato_sauce 0.69 $grains item price 1 bread 2.99 2 pasta 1.99 $dairy item price 1 milk 2.73 2 butter 2.57 Here, we use lists and data frames to create a data set of our grocery list. This list can be traversed depending on what section of the store we find ourselves in. For example, suppose we are in the produce section, and we need to recall what items to buy. We could utilize the following code to remind ourselves. groc$produce Is this grocery list an example of rectangular or non-rectangular data (try to use the definition of rectangular data given above)? Are there examples of rectangular data contained within the grocery list? How could we restructure the data to rectangularize the grocery list? As constructed, this grocery list is an example of non-rectangular data. As a whole, the grocery list is not shaped like a rectangle, but rather, consists of sets of rectangular data, where the sets are defined by the section of the store. Within a section of the store, the items and prices are given in rectangular form since every value is defined by a row and column. While lists are often a useful return object for user-defined functions, they are often troublesome to work with because they may be non-rectangular. If a data set can be restructured or created in rectangular form, it should be. Rectangular data is especially important within the Tidyverse, a self-described opinionated collection of R packages designed for data science. All packages within the Tidyverse rely on the principle of tidy data, data structures where observations are given by rows and variables are given by columns. As defined, tidy data are rectangular, so as we embark on wrangling, visualizing, and modeling data in future chapters, it is important to ponder the nature of our data and whether it can be rectangularized. Create your own example of non-rectangular data. Make sure your example is assigned to a variable with an appropriate name. Lets consider how we can rectangularize the grocery list. Instead of creating a list of named data frames, where the name represents the section of the store, lets create a grocery list where each row represents an item and columns specify the section and price. Because the Tidyverse requires rectangular data, there are several functions which are handy for converting data structures to rectangular form. We could utilize one of these functions to rectangularize the data set. library(tidyverse, quietly = TRUE) groc_rec &lt;- groc %&gt;% bind_rows(., .id = &#39;section&#39;) groc_rec The code chuck above is an example of piping, a concept used heavily in the tidyverse which allows the easy chaining of actions, where the output of one function is piped to the input argument of the next function. Here the groc list is being piped into the first argument (denoted with a .) of the bind_rows function. Though it may look confusing at first, piping is a nice way to increase code readability, since the order that functions will be applied matches the order they appear in the code (from top to bottom). For more information, see here. Or, we can simply create the grocery list in rectangular form to begin with. Create a data frame called groc_df which contains the above grocery list in rectangular form. Any feedback for this section? Click here 2.1.1 Reading and Writing Rectangular Data Rectangular data are often stored locally using text files (.txt), comma separated value files (.csv), and Excel files (.xlsx). When data are written to these file types, they are easy to view across devices, without the need for R. Since most grocery store trips obviate the need for R, lets consider how to write our grocery list to each of these file types. To write to and read from data text files or comma separated value files, the readr package will come in handy, whereas the xlsx package will allow us to write to and read from Excel files. To write data from R to a file, we will leverage commands beginning with write. # text file readr::write_delim(groc_rec, path = &#39;data_raw/groceries-rectangular.txt&#39;) # csv file readr::write_csv(groc_rec, path = &#39;data_raw/groceries-rectangular.csv&#39;) # Excel file xlsx::write.xlsx(groc_rec, file = &#39;data_raw/groceries-rectangular.xlsx&#39;, row.names = FALSE) # don&#39;t include the row names To read data from a file to R, we will leverage commands beginning with read. Before reading data into R, you will need to look at the file and file extension to better understand which function to use. # text file readr::read_delim(&#39;data_raw/groceries-rectangular.txt&#39;, delim = &#39; &#39;, lazy = FALSE) # the file delimits columns using a space # csv file readr::read_csv(&#39;data_raw/groceries-rectangular.csv&#39;, lazy = FALSE) # Excel file xlsx::read.xlsx(&#39;data_raw/groceries-rectangular.xlsx&#39;, sheetName = &#39;Sheet1&#39;) # load the sheet called &#39;Sheet1&#39; Reading files into R can sometimes be frustrating. Always look at the data to see if there are column headers and row names. Text files can have different delimiters, characters which separate values in a data set. The default delimiter for readr::write_delim() is a space, but other common text delimiters are tabs, colons, semi-colons, or vertical bars. Commas are so commonly used as a delimiter, it gets a function of its own (.csv). Always ensure that data from an Excel spreadsheet are rectangular. Lastly, the readr package will guess the data type of each column. Check these data types are correct using str(). Reading and writing csv and text files can also be done with the read.csv, write.csv, read.table and write.table functions included in base R. However, the readr package belongs to the tidyverse and has additional capabilities (see [here]{https://readr.tidyverse.org} for more info). Write the groc_df data frame that you created above to your computer using the write_delim function and using the write_csv function, using different file names. Open the files using a text editor (not R) and comment on the differences in formatting between the between them. Any feedback for this section? Click here 2.1.2 Reading and Writing Non-rectangular Data Writing non-rectangular data from R to your local machine is easy with the help of write_rds() from the readr package. While the origin of RDS is unclear, some believe it stands for R data serialization. Nonetheless, RDS files store single R objects, regardless of the structure. This means that RDS files are a great choice for data which cannot be written to rectangular file formats such as text, csv, and Excel files. The sister function entitled read_rds() allows you to read any RDS file directly into your current R environment, assuming the file already exists. Similar to RDS files, there are also RData files which can store multiple R objects. These files can be written from R to your local machine using save() and read from your local machine to R using load(). We recommend avoiding RData files, and instead, storing multiple R objects in one named list which is then saved as an RDS file. When there is inevitably non-rectangular data that exist which you would like to load into R, you are in for a treat. The rest of this module can loosely be viewed as a guide to managing and curating data. We will leverage many tools to tackle this problem, but in the next two sections, we will address two specific, common instances of non-rectangular data: data from APIs and from scraped sources. Write the non-rectangular data that you created above to your computer using the write_rds() function. Remember that you can use ?write_rds() to learn more about the function. Any feedback for this section? Click here "],["APIs.html", "2.2 APIs: Clean and Curated", " 2.2 APIs: Clean and Curated An application programming interface (API) is a set of functions and procedures which allows one computer program to interact with another. To simplify the concept remarkably, we will consider web-APIs where there is a server called a host (computer waiting to provide data) and a client (computer making a request for data). The benefit of APIs is the result: clean and curated data from the host. The pre-processing needed to get the data in a workable form is usually already done on the server side. We, however, are responsible for making the request. Web-APIs often utilize JavaScript Object Notation (JSON), another example of non-rectangular data. We will utilize the httr and the jsonlite packages to retrieve the latest sports lines from Bovada, an online sportsbook. Before we start, well need to download the httr and jsonlite packages and load them into our current environment. Furthermore, we will need to find the address of the server to which we will send the request. library(httr, quietly = TRUE) # quietly means don&#39;t print any output when loading the library library(jsonlite, quietly = TRUE) # URL for api requests: bov_nfl_api &lt;- &quot;https://www.bovada.lv/services/sports/event/v2/events/A/description/football/nfl&quot; To ask for data through a web-API, we will need to make a GET request with the httr packages GET() function. After making the request, we can read about the servers response. bov_req &lt;- httr::GET(url = bov_nfl_api) bov_req Response [https://www.bovada.lv/services/sports/event/v2/events/A/description/football/nfl] Date: 2021-11-12 22:15 Status: 200 Content-Type: application/json;charset=utf-8 Size: 2.21 MB If the request was successful, then the status of the request will read 200. Otherwise, there was some error with your request. For a list of HTTP status codes and their respective definitions, follow this link. Since the response clarifies that the content is indeed driven by JavaScript, then we will utilize the jsonlite package to read the JSON structured data. A handy function we will use will be fromJSON() which converts a character vector containing data in JSON structure to native structures in R like lists. So, in order, we will Extract the content from the servers response Convert the content to a character vector, maintaining the JSON structure Restructure the data into native R structures, using fromJSON(). content &lt;- bov_req$content content_char &lt;- rawToChar(content) bov_res &lt;- jsonlite::fromJSON(content_char) The website icanhazdadjoke.com provides an API so you can fetch a random dad joke. Following the Bovada example above, use httr::GET to retrieve a joke, where the url is icanhazdadjoke.com. Unlike the example above, you will need to add another argument to the GET function: add_headers(Accept=application/json), which tells the website to send its response in JSON format. Next, extract the content from the servers response, convert it to a character string, and use jsonlite::fromJSON to parse the JSON structure. Of course, we could also create a function which takes the servers response and converts the content to native R structures. We will want to code in a force stop if the response status is not 200. We will also want to require the httr and jsonlite packages which will automatically install the packages if a user calls the function without having the packages installed. convert_JSON &lt;- function(resp){ # call needed packages require(httr) require(jsonlite) # stop if the server returned an error httr::stop_for_status(resp) # return JSON content in native R structures return(jsonlite::fromJSON(rawToChar(resp$content))) } Finally, we can get the same output by simply calling the function. identical(convert_JSON(bov_req), bov_res) [1] TRUE Write code to retrieve 100 random dad jokes from icanhazdadjoke.com, and store the result in a list called jokes. Recall from Module 1 that you can do this with a for-loop, or with the lapply function. Next, count how many of the random dad jokes you retrieved contain the word Why (with a capital W). Heres a hint to get you started: use grepl(Why, jokes) to produce a binary vector, which elements are TRUE if Why occurs in the joke, and FALSE otherwise. Then count the TRUE elements using sum. Some web-APIs require additional information from us as outlined in the documentation for the API. In this case, the user would need to provide additional query parameters in their GET request. Thankfully, this functionality is ingrained in the httr packages GET() function. For more information on how to include query parameters, type ??GET into your R console. Any feedback for this section? Click here "],["scraping-messy-and-mangled.html", "2.3 Scraping: Messy and Mangled", " 2.3 Scraping: Messy and Mangled If you are reading this textbook, at some point in your career, you are likely to want or need data which exists on the web. You have looked for downloadable sources and Google searched for an API, but alas, no luck. The last resort for importing data into R is web scraping. Web scraping is a technique for harvesting data which is portrayed on the web and exists in hypertext markup language (HTML), the language of web browser documents. 2.3.1 Scraping vs APIs The benefit of using an API are clean data. For example, we can traverse the result to find the latest NFL events. head(bov_res[[2]][[1]][,2]) [1] &quot;Atlanta Falcons @ Dallas Cowboys&quot; [2] &quot;Buffalo Bills @ New York Jets&quot; [3] &quot;Cleveland Browns @ New England Patriots&quot; [4] &quot;Detroit Lions @ Pittsburgh Steelers&quot; [5] &quot;Jacksonville Jaguars @ Indianapolis Colts&quot; [6] &quot;New Orleans Saints @ Tennessee Titans&quot; With more digging, we can find which teams are playing at home. head(bov_res[[2]][[1]][[16]]) [[1]] id name home 1 9420754-11904234 Dallas Cowboys TRUE 2 9420754-11904242 Atlanta Falcons FALSE [[2]] id name home 1 9420740-11904241 New York Jets TRUE 2 9420740-11904215 Buffalo Bills FALSE [[3]] id name home 1 9420738-11852810 New England Patriots TRUE 2 9420738-11904219 Cleveland Browns FALSE [[4]] id name home 1 9420743-11904223 Pittsburgh Steelers TRUE 2 9420743-11904244 Detroit Lions FALSE [[5]] id name home 1 9420734-11904232 Indianapolis Colts TRUE 2 9420734-11904220 Jacksonville Jaguars FALSE [[6]] id name home 1 9420742-11904229 Tennessee Titans TRUE 2 9420742-11904221 New Orleans Saints FALSE We can also find the current line of each of these games. Here, I have created a function called get_bovada_lines() which traverses this complicated (yet clean) JSON object using methods explored in Chapter 3 and combines the information together into a rectangular data set. bov_res %&gt;% get_bovada_lines() While traversing these sometimes complicated lists may seem intimidating, with practice, working with data from an API will be made easier after discussing mapping functions in Chapter 3 which are useful for traversing complicated lists. Hopefully, after the scraping section, you will find working with APIs like a walk in the park compared to scraping data directly from the web. 2.3.2 Lessons Learned from Scraping Scraping is a necessary evil that requires patience. While some tasks may prove easy, you will quickly find others seem insurmountable. In this section, we will outline a few tips to help you become a web scraper. Brainstorm! Before jumping into your scraping project, ask yourself what data do I need and where can I find it? If you discover you need data from various sources, what is the unique identifier, the link which ties these data together? Taking the time to explore different websites can save you a vast amount of time in the long run. As a general rule, simplistic looking websites are generally easier to scrape and often contain the same information as more complicated websites with several bells and whistles. Start small! Sometimes a scraping task can feel daunting, but it is important to view your project as a war, splitting it up into small battles. If you are interested in the racial demographics of each of the United States, consider how you can first scrape this information for one state. In this process, dont forget tip 1! Hyperlinks are your friend! They can lead to websites with more detailed information or serve as the unique identifier you need between different data sources. Sometimes you wont even need to scrape the hyperlinks to navigate between webpages, making minor adjustments to the web address will sometimes do. Data is everywhere! Text color, font, or highlighting may serve as valuable data that you need. If these features exist on the webpage, then they exist within the HTML code which generated the document. Sometimes these features are well hidden or even inaccessible, leading to the last and final tip. Ready your search engine! Just like coding in R is an art, web developing is an art. When asking distinct developers to create the same website with the same functionality, the final result may be similar but the underlying HTML code could be drastically different. Why does this matter? You will run into an issue that hasnt been addressed in this text. Thankfully, if youve run into an issue, someone else probably has too. We cannot recommend websites like Stack Overflow enough. 2.3.3 Tools for Scraping Before we can scrape information from a webpage, we need a bit of background on how this information is stored and presented. The goal of this subsection is to briefly introduce the languange of the web, hypertext markup language (HTML). When we talk about scraping the web, what we really mean is gathering bits of information from the HTML code used to build a webpage. Like R code, HTML can be overwhelming. The goal is not to teach HTML but to introduce its components, so you have a much more intuitive sense of what we are doing when we scrape the web. 2.3.3.1 Hypertext Markup Language (HTML) Web sites are written in hypertext markup language. All contents that are displayed on a web page are structured through HTML with the help of HTML elements. HTML elements consist of a tag and contents. The tag defines how the web browser should format and display the content. Aptly, the content is what should be displayed. For example, if we wished to format text as a paragraph within the web document, then we could use the paragraph tag, &lt;p&gt;, to indicate the beginning of a paragraph. After opening a tag, we then specify the content to display before closing the tag. A complete paragraph may read: &lt;p&gt; This is the paragraph you want to scrape. &lt;/p&gt; Attributes are optional parameters which provide additional information about the element in which the attribute is included. For example, within the paragraph tag, you can define a class attribute which formats the text in a specific way, such as bolding, coloring, or aligning the text. To extend our example, the element may read: &lt;p class = \"fancy\"&gt; This is the paragraph you want to scrape which has been formatted in a fancy script. &lt;/p&gt; The type of attribute, being class, is the attribute name, whereas the quantity assigned to the attribute, being fancy, is the attribute value. The general decomposition of an HTML element is characterized by the following figure: Figure 2.1: the lingo of an HTML element The class attribute is a flexible one. Many web developers use the class attribute to point to a class name in a style sheet or to access and manipulate elements with the specific class name with a JavaScript. For more information of the class attribute, see this link. For more information on cascading style sheets which are used to decorate HTML pages, see this link. Any feedback for this section? Click here 2.3.3.2 Selector Gadgets While all web pages are composed of HTML elements, the elements themselves can be structured in complicated ways. Elements are often nested inside one another or make use of elements in other documents. These complicated structures can make scraping data difficult. Thankfully, we can circumvent exploring these complicated structures with the help of selector gadgets. A selector gadget allows you to determine what css selector you need to extract the information desired from a webpage. These JavaScript bookmarklets allow you to determine where the information you desire belongs within the complicated structure of elements that makeup a webpage. To follow along in Chapter 3, you will need to download one of these gadgets from this link. If you use Google Chrome, you can download the bookmark extension directly from this link. If the selector gadget fails us, we can always view the structure of the elements directly by viewing the page source. This can be done by right-clicking on the webpage and selecting View Page Source. For Google Chrome, you can also use the keyboard shortcut CTRL-U. 2.3.4 Scraping NFL Data In Chapter 2.2, we gathered some betting data pertaining to the NFL through a web-API. We may wish to supplement these betting data with data pertaining to NFL teams, players, or even playing conditions. The goal in this subsection is to introduce you to scraping by heeding the advice given in the Chapter 2.3.2. Further examples are given in the supplemental material. Following our own advice, lets brainstorm. When you think of NFL data, you probably think of NFL.com or ESPN. These sites obviously have reliable data, but the webpages are pretty involved. While the filters, dropdown menus, and graphics lend great experiences for web browsers, they create headaches for web scrapers. After further digging, we will explore Pro Football Reference, a reliable archive for football statistics (with a reasonably simple webpage). This is an exhaustive source which boasts team statistics, player statistics, and playing conditions for various seasons. Lets now start small by focusing on team statistics, but further, lets limit our scope to the 2020 Denver Broncos. Notice, there are hyperlinks for each player documented in any of the categories, as well hyperlinks for each games boxscore where there is information about playing conditions and outcomes. Hence, we have a common thread between team statistics, players, and boxscores. If, for example, we chose to scrape team statistics from one website and player statistics from another website, we may have to worry about a unique identifier (being team) if the websites have different naming conventions. 2.3.4.1 HTML Tables: Team Statistics Well start with the team statistics for the 2020 Denver Broncos which can be found in a table entitled Team Stats and Rankings. Well need to figure in which element or node the table lives within the underlying HTML. To do this, we will utilize the CSS selector gadget. If we highlight over and click the table with the selector gadget, we will see that the desired table lives in an element called #team_stats. Figure 2.2: finding the team statistics element using the selector gadget Alternatively, we could view the page source and search for the table name. Ive highlighted the information identified by the selector gadget with the cursor. Figure 2.3: finding the team statistics element using the page source While the selector gadget is always a great first option, it is not always reliable. There are instances when the selector gadget identifies a node that is hidden or inaccessible without JavaScript. In these situations, it is best view the page source directly for more guidance on how to proceed. Practice with both the selector gadget and the page source. Once we have found the name of the element containing the desired data, we can utilize the rvest package to scrape the table. The general process for scraping an HTML table is Read the HTML identified by the web address. Isolate the node containing the data we desire. Parse the HTML table. Take a look at the data to ensure the columns are appropriate labels. library(rvest) library(janitor) pfr_url &lt;- &quot;https://www.pro-football-reference.com&quot; broncos_url &lt;- str_c(pfr_url, &#39;/teams/den/2020.htm&#39;) broncos_url %&gt;% # read the HTML read_html(.) %&gt;% # isolate the node containing the HTML table html_node(., css = &#39;#team_conversions&#39;) %&gt;% # parse the html table html_table(.) %&gt;% # make the first row of the table column headers and clean up column names row_to_names(., row_number = 1) %&gt;% clean_names() While these data need cleaning up before they can be used in practice, we will defer these responsibilities to Chapter 3. Take this time to scrape the Team Conversions table on your own. While it is exciting to scrape your first nuggets of data, we have just scratched the surface of web scraping. To be honest, it took some time to find data this easy to scrape. More often than not, difficulties arise. The HTML table you want may be commented out or hidden. Your data may not be in the form of a table, at all. For a more in-depth exposition of web scraping, see the supplemental materials. Any feedback for this section? Click here "],["Wrangling.html", "Chapter 3 Wrangling", " Chapter 3 Wrangling The work that you do with data wrangling others would call data plumbing or even janitorial work, but when you have somebody who knows how to wrangle data and gets into a flow of data wrangling, its an elegant dance to watch. Stephanie McReynolds, Strategic Adviser, Nexla In Chapter 2, we introduced the idea of rectangular data vs. non-rectangular data, providing examples for each and demonstrating the process of rectangularization. We outlined how to use a web-API before a light introduction to web scraping. In this chapter, we will familiarize ourselves with data wrangling, the art of cleaning up our data. Furthermore, in Chapter 2, we briefly touched on the notion of tidy data: data structures where observations are given by rows, variables are given by columns, and values are given by cells. The notion of tidy data was formalized by Wickham and others (2014), Chief Scientist at RStudio and creator of the Tidyverse universe, to reduce the amount of work involved in preparing data. Data preparation, or data cleaning, is often a time consuming task with real data. Since its necessary to format data as tidy data to use the vast network of packages within the Tidyverse, we always need to first structure our data as tidy data. Figure 3.1: visualizing tidy data In the following subsections, we will be cleaning up those data gathered from Pro Football Reference. Any feedback for this section? Click here References "],["core-tidyverse.html", "3.1 Core Tidyverse", " 3.1 Core Tidyverse At the heart of Tidyverse are a few packages: dplyr, tidyr, stringr, and forcats. Each package serves a powerful purpose. tidyr helps you create tidy data, while dplyr is used for data manipulation. Think of tidyr as the crowbar or saw in your toolbox, allowing you to bend and shape your data into tidy shape. dplyr, on the other hand, is more like the screwdriver, hammer, or level, allowing you to fix pesky issues with the data. stringr and forcats are useful when working with strings and factors (categorical variables that have a fixed and known set of possible values). While we will make the effort to teach tidyr and dplyr separately within their own subsections, we recognize that these packages are ultimately created to be used together. Since we may need to make use of functions across packages, we will explicitly state the origin each function by package::function(). 3.1.1 tidyr Lets take a look at the game by game player rushing and receiving statistics that we scraped using the principles outlined in the previous chapter. To do this, we will first use stringr::str_c(), a function weve seen a few times now, to create a web addresses corresponding to Pro Football Reference page for all of the 2020 NFL teams. pfr_url &lt;- &quot;https://www.pro-football-reference.com&quot; team_urls &lt;- pfr_url %&gt;% # get team abbreviations for all NFL teams and isolate Denver get_teams(.) %&gt;% # create URLs for all 2020 NFL teams stringr::str_c(pfr_url, ., &#39;2020.htm&#39;) as_tibble(team_urls) Pipes (%&gt;%) make your code much more readable and avoid unnecessary assignments. While not required in many cases, I use a period to indicate where the result to the left of the pipe belongs in the argument to the right of the pipe. Now that we have the web addresses for every 2020 NFL team, lets isolate the 2020 Denver Broncos and scrape their players aggregated rushing and receiving statistics. We will use dplyr::glimpse() to see the data types and a few values for each column in the data set. den_stats &lt;- team_urls %&gt;% # isolate the 2020 Denver Bronco&#39;s URL .[10] %&gt;% # get team statistics for 2020 Denver Broncos get_team_stats() dplyr::glimpse(den_stats) Rows: 23 Columns: 28 $ team &lt;chr&gt; &quot;den&quot;, &quot;den&quot;, &quot;den&quot;, &quot;den&quot;, &quot;den&quot;, &quot;den&quot;, &quot;den&quot;~ $ no &lt;chr&gt; &quot;25&quot;, &quot;30&quot;, &quot;3&quot;, &quot;28&quot;, &quot;13&quot;, &quot;9&quot;, &quot;4&quot;, &quot;32&quot;, &quot;1~ $ player &lt;chr&gt; &quot;Melvin Gordon&quot;, &quot;Phillip Lindsay&quot;, &quot;Drew Lock&quot;~ $ age &lt;chr&gt; &quot;27&quot;, &quot;26&quot;, &quot;24&quot;, &quot;24&quot;, &quot;21&quot;, &quot;27&quot;, &quot;24&quot;, &quot;24&quot;,~ $ pos &lt;chr&gt; &quot;RB&quot;, &quot;rb&quot;, &quot;QB&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;~ $ games_g &lt;chr&gt; &quot;15&quot;, &quot;11&quot;, &quot;13&quot;, &quot;16&quot;, &quot;13&quot;, &quot;3&quot;, &quot;3&quot;, &quot;5&quot;, &quot;1~ $ games_gs &lt;chr&gt; &quot;10&quot;, &quot;8&quot;, &quot;13&quot;, &quot;0&quot;, &quot;4&quot;, &quot;1&quot;, &quot;1&quot;, &quot;0&quot;, &quot;0&quot;, ~ $ rushing_att &lt;chr&gt; &quot;215&quot;, &quot;118&quot;, &quot;44&quot;, &quot;35&quot;, &quot;9&quot;, &quot;6&quot;, &quot;5&quot;, &quot;4&quot;, &quot;~ $ rushing_yds &lt;chr&gt; &quot;986&quot;, &quot;502&quot;, &quot;160&quot;, &quot;170&quot;, &quot;40&quot;, &quot;28&quot;, &quot;-5&quot;, &quot;~ $ rushing_td &lt;chr&gt; &quot;9&quot;, &quot;1&quot;, &quot;3&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0~ $ rushing_lng &lt;chr&gt; &quot;65&quot;, &quot;55&quot;, &quot;16&quot;, &quot;23&quot;, &quot;15&quot;, &quot;9&quot;, &quot;-1&quot;, &quot;8&quot;, &quot;~ $ rushing_y_a &lt;chr&gt; &quot;4.6&quot;, &quot;4.3&quot;, &quot;3.6&quot;, &quot;4.9&quot;, &quot;4.4&quot;, &quot;4.7&quot;, &quot;-1.0~ $ rushing_y_g &lt;chr&gt; &quot;65.7&quot;, &quot;45.6&quot;, &quot;12.3&quot;, &quot;10.6&quot;, &quot;3.1&quot;, &quot;9.3&quot;, &quot;~ $ rushing_a_g &lt;chr&gt; &quot;14.3&quot;, &quot;10.7&quot;, &quot;3.4&quot;, &quot;2.2&quot;, &quot;0.7&quot;, &quot;2.0&quot;, &quot;1.~ $ rushing_fmb &lt;chr&gt; &quot;4&quot;, &quot;0&quot;, &quot;8&quot;, &quot;0&quot;, &quot;2&quot;, &quot;0&quot;, &quot;1&quot;, &quot;0&quot;, &quot;2&quot;, &quot;0~ $ receiving_tgt &lt;chr&gt; &quot;44&quot;, &quot;14&quot;, &quot;&quot;, &quot;13&quot;, &quot;56&quot;, &quot;&quot;, &quot;&quot;, &quot;1&quot;, &quot;6&quot;, &quot;~ $ receiving_rec &lt;chr&gt; &quot;32&quot;, &quot;7&quot;, &quot;&quot;, &quot;12&quot;, &quot;30&quot;, &quot;&quot;, &quot;&quot;, &quot;1&quot;, &quot;3&quot;, &quot;&quot;~ $ receiving_yds &lt;chr&gt; &quot;158&quot;, &quot;28&quot;, &quot;&quot;, &quot;81&quot;, &quot;381&quot;, &quot;&quot;, &quot;&quot;, &quot;5&quot;, &quot;26&quot;~ $ receiving_y_r &lt;chr&gt; &quot;4.9&quot;, &quot;4.0&quot;, &quot;&quot;, &quot;6.8&quot;, &quot;12.7&quot;, &quot;&quot;, &quot;&quot;, &quot;5.0&quot;,~ $ receiving_td &lt;chr&gt; &quot;1&quot;, &quot;0&quot;, &quot;&quot;, &quot;0&quot;, &quot;3&quot;, &quot;&quot;, &quot;&quot;, &quot;0&quot;, &quot;0&quot;, &quot;&quot;, &quot;~ $ receiving_lng &lt;chr&gt; &quot;20&quot;, &quot;11&quot;, &quot;&quot;, &quot;28&quot;, &quot;49&quot;, &quot;&quot;, &quot;&quot;, &quot;5&quot;, &quot;10&quot;, ~ $ receiving_r_g &lt;chr&gt; &quot;2.1&quot;, &quot;0.6&quot;, &quot;&quot;, &quot;0.8&quot;, &quot;2.3&quot;, &quot;&quot;, &quot;&quot;, &quot;0.2&quot;, ~ $ receiving_y_g &lt;chr&gt; &quot;10.5&quot;, &quot;2.5&quot;, &quot;&quot;, &quot;5.1&quot;, &quot;29.3&quot;, &quot;&quot;, &quot;&quot;, &quot;1.0&quot;~ $ receiving_ctch_percent &lt;chr&gt; &quot;72.7%&quot;, &quot;50.0%&quot;, &quot;&quot;, &quot;92.3%&quot;, &quot;53.6%&quot;, &quot;&quot;, &quot;&quot;,~ $ receiving_y_tgt &lt;chr&gt; &quot;3.6&quot;, &quot;2.0&quot;, &quot;&quot;, &quot;6.2&quot;, &quot;6.8&quot;, &quot;&quot;, &quot;&quot;, &quot;5.0&quot;, ~ $ yds_touch &lt;chr&gt; &quot;247&quot;, &quot;125&quot;, &quot;44&quot;, &quot;47&quot;, &quot;39&quot;, &quot;6&quot;, &quot;5&quot;, &quot;5&quot;, ~ $ yds_y_tch &lt;chr&gt; &quot;4.6&quot;, &quot;4.2&quot;, &quot;3.6&quot;, &quot;5.3&quot;, &quot;10.8&quot;, &quot;4.7&quot;, &quot;-1.~ $ yds_y_scm &lt;chr&gt; &quot;1144&quot;, &quot;530&quot;, &quot;160&quot;, &quot;251&quot;, &quot;421&quot;, &quot;28&quot;, &quot;-5&quot;,~ There are a few things to note after looking at the data, but lets first consider if these data are tidy. To answer this question, we need more context. If we wished to predict a players position based on his statistics, then each player is an observation and his statistics are variables. In this case, these data are tidy in their current wide form, when each player has information in only one row. If we are interested in comparing players across the recorded statistical categories, then an observation would no longer be a player but a players statistical category. In this case, these data are tidy in long form, when each player has information in multiple rows. No matter the context, tidyr makes it easy convert your data from wide form to long form with tidyr::pivot_longer() and from long form to wide form with tidyr::pivot_wider(). Figure 3.2: visualizing the transformation from wide data to long data, and vice versa Lets consider how we can use tidyr::pivot_longer() to tidy these data in preparation for comparing players across their statistical categories. In each row, we should expect to have (1) the players name, age, and position, (2) the statistical category, and (3) the value of the statistical category. Imagine taking every other column and pushing them into the rows, duplicating the information above as needed. To do this in R, we will use the dplyr functions dplyr::vars() which allows us to select which variables to push into the rows and dplyr::starts_with() which allows us to pick only variables whose names start with some string. These are two handy functions to know. Further, we will use tidyr::seperate to split the statistical categories into a more general category (i.e. rushing, receiving, etc) and a statistic (i.e. yards per rushing attempt, catch percentage, etc). den_stats_long &lt;- den_stats %&gt;% # push columns into the rows, renaming the names and values columns pivot_longer(., cols = c(starts_with(&#39;games_&#39;), starts_with(&#39;rushing_&#39;), starts_with(&#39;receiving_&#39;), starts_with(&#39;yds_&#39;)), names_to = &#39;stat_category&#39;, values_to = &#39;value&#39;) %&gt;% # seperate the stat category into a category column and a stat column separate(., col = &#39;stat_category&#39;, into = c(&#39;category&#39;, &#39;stat&#39;), sep = &#39;_&#39;, extra = &#39;merge&#39;) den_stats_long If we are in a situation when we are given a data set in long form, but we need it in wide form, we can use tidyr::pivot_wider. den_stats_wide &lt;- den_stats_long %&gt;% pivot_wider(., names_from = c(&#39;category&#39;, &#39;stat&#39;), values_from = &#39;value&#39;) den_stats_wide After converting back to wide form, the result is the same as the original data set. 3.1.2 dplyr Weve addressed how to change the shape of your data using tidyr. Now, we will transition into dplyr where we will outline some of the many functions that can prove helpful in preparing your data for plotting or modeling. Before we jump into various functions, lets outline the issues with our data set which have not been addressed. The best way to get a sense of the issues is to look at the data. den_stats str(den_stats) tibble [23 x 28] (S3: tbl_df/tbl/data.frame) $ team : chr [1:23] &quot;den&quot; &quot;den&quot; &quot;den&quot; &quot;den&quot; ... $ no : chr [1:23] &quot;25&quot; &quot;30&quot; &quot;3&quot; &quot;28&quot; ... $ player : chr [1:23] &quot;Melvin Gordon&quot; &quot;Phillip Lindsay&quot; &quot;Drew Lock&quot; &quot;Royce Freeman&quot; ... $ age : chr [1:23] &quot;27&quot; &quot;26&quot; &quot;24&quot; &quot;24&quot; ... $ pos : chr [1:23] &quot;RB&quot; &quot;rb&quot; &quot;QB&quot; &quot;&quot; ... $ games_g : chr [1:23] &quot;15&quot; &quot;11&quot; &quot;13&quot; &quot;16&quot; ... $ games_gs : chr [1:23] &quot;10&quot; &quot;8&quot; &quot;13&quot; &quot;0&quot; ... $ rushing_att : chr [1:23] &quot;215&quot; &quot;118&quot; &quot;44&quot; &quot;35&quot; ... $ rushing_yds : chr [1:23] &quot;986&quot; &quot;502&quot; &quot;160&quot; &quot;170&quot; ... $ rushing_td : chr [1:23] &quot;9&quot; &quot;1&quot; &quot;3&quot; &quot;0&quot; ... $ rushing_lng : chr [1:23] &quot;65&quot; &quot;55&quot; &quot;16&quot; &quot;23&quot; ... $ rushing_y_a : chr [1:23] &quot;4.6&quot; &quot;4.3&quot; &quot;3.6&quot; &quot;4.9&quot; ... $ rushing_y_g : chr [1:23] &quot;65.7&quot; &quot;45.6&quot; &quot;12.3&quot; &quot;10.6&quot; ... $ rushing_a_g : chr [1:23] &quot;14.3&quot; &quot;10.7&quot; &quot;3.4&quot; &quot;2.2&quot; ... $ rushing_fmb : chr [1:23] &quot;4&quot; &quot;0&quot; &quot;8&quot; &quot;0&quot; ... $ receiving_tgt : chr [1:23] &quot;44&quot; &quot;14&quot; &quot;&quot; &quot;13&quot; ... $ receiving_rec : chr [1:23] &quot;32&quot; &quot;7&quot; &quot;&quot; &quot;12&quot; ... $ receiving_yds : chr [1:23] &quot;158&quot; &quot;28&quot; &quot;&quot; &quot;81&quot; ... $ receiving_y_r : chr [1:23] &quot;4.9&quot; &quot;4.0&quot; &quot;&quot; &quot;6.8&quot; ... $ receiving_td : chr [1:23] &quot;1&quot; &quot;0&quot; &quot;&quot; &quot;0&quot; ... $ receiving_lng : chr [1:23] &quot;20&quot; &quot;11&quot; &quot;&quot; &quot;28&quot; ... $ receiving_r_g : chr [1:23] &quot;2.1&quot; &quot;0.6&quot; &quot;&quot; &quot;0.8&quot; ... $ receiving_y_g : chr [1:23] &quot;10.5&quot; &quot;2.5&quot; &quot;&quot; &quot;5.1&quot; ... $ receiving_ctch_percent: chr [1:23] &quot;72.7%&quot; &quot;50.0%&quot; &quot;&quot; &quot;92.3%&quot; ... $ receiving_y_tgt : chr [1:23] &quot;3.6&quot; &quot;2.0&quot; &quot;&quot; &quot;6.2&quot; ... $ yds_touch : chr [1:23] &quot;247&quot; &quot;125&quot; &quot;44&quot; &quot;47&quot; ... $ yds_y_tch : chr [1:23] &quot;4.6&quot; &quot;4.2&quot; &quot;3.6&quot; &quot;5.3&quot; ... $ yds_y_scm : chr [1:23] &quot;1144&quot; &quot;530&quot; &quot;160&quot; &quot;251&quot; ... The first issue with the data set that we should address is the inappropriate data types assigned to each column. Every column is scraped as a character, but the players position (pos) should be coded as a factor variable and the players statistics should be coded as numeric variables. Before we can change the players catch percentage (receiving_ctch_percent) to a numeric variable, we first need to remove the percent sign from the values of the variable. 3.1.2.1 mutate When we wish to change the values within a column, we can leverage the dplyr::mutate() function. Lets see how we can use some stringr functions within dplyr::mutate() to clean up some of these columns. den_stats_wking &lt;- den_stats %&gt;% mutate(., age = as.numeric(age), pos = pos %&gt;% str_to_upper() %&gt;% na_if(., &#39;&#39;) %&gt;% as_factor(), receiving_ctch_percent = str_remove_all(receiving_ctch_percent, &#39;%&#39;)) den_stats_wking Within one mutate() call, we (1) recoded the players age as a numeric variable, (2) changed the players position to uppercase, replaced empty strings with NA values, and recoded the result as a factor, and (3) removed all percent signs from the catch percent statistic. We still need to change all of the players statistics to numeric variable. We could do this similarly to the players age, but listing out every variable one by one would be painstakingly inefficient. To complete this task, we can use a common variant of the mutate() function called mutate_at() to change every specified column in a similar manner. We can then use mutate_all() to replace every missing string with an NA value, the proper way to specify a missing value in R. den_stats_wking &lt;- den_stats_wking %&gt;% mutate_at(., vars(starts_with(&#39;games_&#39;), starts_with(&#39;rushing_&#39;), starts_with(&#39;receiving_&#39;), starts_with(&#39;yds_&#39;)), ~as.numeric(.x)) %&gt;% mutate_all(., ~na_if(.x, &#39;&#39;)) These variants of mutate() (e.g. mutate_at and mutate_all) are aimed to condense code when the same transformation is applied to many columns. To specify the transformation which will be applied to all of the identified columns, you will need a lambda expression. The lambda expressions start with ~ and are followed by a function specifying the transformation such as as.numeric. One of the arguments of the function will be .x which indicates an arbitrary column. To break down the mutate_at() specified above, we seek to transform all columns starting with games_, rushing_, receiving_, and yds_. Overall, there are 23 columns we will be transforming with this single mutate function. The lambda expression ~as.numeric(.x) specifies how to transform each of these 23 columns. That is, for a given column called .x, change .x to be a numeric column. For more information on these variants, also known as scoped verbs (e.g. _if, _at, _all), type ?mutate_if into the console. Several functions we will discuss throughout this chapter also can be used with one of these scoped verbs. 3.1.2.2 slice and filter Notice, at the bottom of the data set there are two rows which summarize the table. We do not need these rows since they do not outline the performance of an individual player. Keeping these rows would violate the principle of tidy data. There are two approaches we can take to remedy this issue. First, we could consider removing the last two rows of the data set or keeping all rows except for the last two. When we would like to remove or keep rows of a data set using the index of the row, we can leverage the slice() function. To illustrate how to do this, we will use a new helper function n() which returns the number of rows in the referenced data set. # option 1: remove the last two rows of the data set den_stats_wking %&gt;% slice(., -(n()-1), -n()) # or den_stats_wking %&gt;% slice(., -c(n()-1, n())) # option 2: keep all rows except for the last two den_stats_wking %&gt;% slice(., 1:(n()-2)) While this certainly solves the issue for this particular data set, it is not a robust solution. If we hope to apply this same logic for other NFL teams, we need to recognize that this solution relies on there only being two total columns which take up the last two rows of the data set. A more robust solution would be to keep only rows consisting of a player not named \"Team Total\" or \"Opp Total\". If we want to choose rows based on specific criteria, then we can utilize filter() to choose rows based on specific criteria. den_stats_wking &lt;- den_stats_wking %&gt;% filter(., !(player %in% c(&#39;Team Total&#39;, &#39;Opp Total&#39;))) den_stats_wking 3.1.2.3 select Suppose were interested in only rushing and receiving statistics for each of the players.. In this case, we may wish to keep columns containing information on the players and the players rushing and receiving statistics. If we want to keep certain columns based on the column names, then we can utilize the select() function. A verbose (but acceptable) solution would be to specify each column we want to keep. To demonstrate, I will keep only information pertaining to the player (e.g. jersey number, name, age, and position). den_stats_wking %&gt;% select(., no, player, age, pos) Since there are several variables for rushing and receiving statistics, we may not want to specify every column name we want to keep. Instead, we can use scoped verbs and helper functions, similar to mutate(). den_stats_wking &lt;- den_stats_wking %&gt;% select_at(., vars(team, no, player, age, pos, starts_with(&#39;rushing_&#39;), starts_with(&#39;receiving_&#39;))) 3.1.2.4 group_by Designed to be used with other functions, group_by groups rows of data set by an attribute. Suppose were interested in positions rather than players. In this case, we would like to group players in common positions. When we invoke the group_by function, we are creating a tibble which looks a lot like the original but indicates which rows belong to which group. den_grouped &lt;- den_stats_wking %&gt;% group_by(., pos) class(den_grouped) group_vars(den_grouped) [1] &quot;grouped_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; [1] &quot;pos&quot; Any functions invoked on grouped data will be conducted on the specified groups, rather than the data set as a whole. For example, suppose were interested in determining the player with the most receptions by potion. We could utilize the helper slice_max() which will return the row with the largest value for the specified attribute. If we forgot to group the data set by position, then we would return the player with the most receptions, clearly a wide receiver. den_stats_wking %&gt;% slice_max(., receiving_rec) If we group by position before invoking slice_max then we would return the players with the most receptions at each of the listed positions. When operating on grouped data, it is important to ungroup when we no longer want operations to be conducted on the groups. den_stats_wking %&gt;% group_by(., pos) %&gt;% slice_max(., receiving_rec) %&gt;% ungroup(.) If grouping data confuses you, dont worry! You are not alone. When using the group_by function, I like to imagine partitioning rows into literal groups and then asking myself what I would like to do with each of the groups. In this case, imagine the coach held a meeting on the field. To start the meeting, he asks all of his running backs (RB) to stand in the home end zone, his wide receivers to stand in the away end zone, and his tight ends (TE) to stand at the home sidelines. All of the other players (with unknown positions) are asked to stand at the away sidelines. In this sense, the coach has created four groups where each group consists of same positioned players. Now, the coach walks to each group individually and asks for the player with the most receptions to follow him. At the end of the day, each group sends one player, so the result is four players. The group_by function is the coach asking his players to organize themselves on the field, and the slice_max function is asking the player with the largest number of receptions to follow him. Since the slice_max follows the group_by function, the coach for a player from each group. 3.1.2.5 summarize The summarize function allows us to calculate summary statistics for specified columns. When paired with group_by, the summarize function allows us to calculate summary statistics for each of the groups. For example, suppose I wanted to calculate the average rushing and receiving yards, as well as standard deviations, for each of non-missing positions except for punter. Similar to the mutate, we can specify how we what we want the new columns to be called and how we want to define them. den_stats_wking %&gt;% filter(., pos !=&#39;P&#39;, !is.na(pos)) %&gt;% group_by(., pos) %&gt;% summarize(., mean_rushing_yds = mean(rushing_yds), sd_rushing_yds = sd(rushing_yds), mean_receiving_yds = mean(receiving_yds), sd_receiving_yds = sd(receiving_yds)) "],["functional-programming-with-purrr.html", "3.2 Functional Programming with purrr", " 3.2 Functional Programming with purrr In the previous subsection, we explored how to reshape and wrangle data with the tidyr and dplyr packages. We broke the functions up into sections for easier cross reference, but in reality, they can (and should) be combined together using pipes to avoid intermediate assignments. The following outlines all steps necessary to clean the rushing and receiving statistics for the 2020 Denver Broncos. &quot;https://www.pro-football-reference.com/teams/den/2020.htm&quot; %&gt;% # scrape team stats get_team_stats(.) %&gt;% # clean results of scrape mutate(., age = as.numeric(age), pos = pos %&gt;% str_to_upper() %&gt;% na_if(., &#39;&#39;) %&gt;% as_factor(), receiving_ctch_percent = str_remove_all(receiving_ctch_percent, &#39;%&#39;)) %&gt;% mutate_at(., vars(starts_with(&#39;games_&#39;), starts_with(&#39;rushing_&#39;), starts_with(&#39;receiving_&#39;), starts_with(&#39;yds_&#39;)), ~as.numeric(.x)) %&gt;% mutate_all(., ~na_if(.x, &#39;&#39;)) %&gt;% filter(., !(player %in% c(&#39;Team Total&#39;, &#39;Opp Total&#39;)), !is.na(pos), pos != &#39;P&#39;) %&gt;% select_at(., vars(team, no, player, age, pos, starts_with(&#39;games_&#39;), starts_with(&#39;rushing_&#39;), starts_with(&#39;receiving_&#39;))) Weve outlined how to get rushing and receiving statistics for the 2020 Denver Broncos, but now, we would like to attain the same statistics for every NFL team. Since Pro-Football Reference reports the same statistics for each team in the same manner, we hope to scrape and clean each teams statistics using the code above, changing only the web address. When we need to iterate over an object, we should reach for the purrr package which contains functions allowing us to map over different elements of a vector, data frame, or list. In our case, we have a vector of webpages, and for each webpage, we would like to scrape the rushing and receiving statistics before cleaning the result. First, we will use the map function to iterate over the vector of webpages. For each webpage, we will invoke get_team_stats. The result is a list of data frames where each element of the list corresponds to a specific teams scraped rushing and receiving statistics. The helper function set_names allows us to specify the name of each element of the resulting list. The lambda function str_sub extracts the three letter code in the team web address which specifies the team. team_stats &lt;- team_urls %&gt;% # create list where each element is the result of the function get_team_stats # applied to the cooresponding map(., ~get_team_stats(.x)) class(team_stats) # print only the first three elements of the list team_stats[1] team_stats[2] [1] &quot;list&quot; [[1]] # A tibble: 20 x 28 team no player age pos games_g games_gs rushing_att rushing_yds &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 crd &quot;41&quot; Kenyan Drake &quot;26&quot; &quot;RB&quot; 15 &quot;13&quot; 239 955 2 crd &quot;1&quot; Kyler Murra~ &quot;23&quot; &quot;QB&quot; 16 &quot;16&quot; 133 819 3 crd &quot;29&quot; Chase Edmon~ &quot;24&quot; &quot;&quot; 16 &quot;2&quot; 97 448 4 crd &quot;15&quot; Chris Strev~ &quot;25&quot; &quot;&quot; 5 &quot;0&quot; 4 15 5 crd &quot;13&quot; Christian K~ &quot;24&quot; &quot;WR&quot; 14 &quot;10&quot; 2 3 6 crd &quot;37&quot; D.J. Foster &quot;27&quot; &quot;&quot; 10 &quot;0&quot; 2 2 7 crd &quot;10&quot; DeAndre Hop~ &quot;28&quot; &quot;WR&quot; 16 &quot;16&quot; 1 1 8 crd &quot;17&quot; Andy Isabel~ &quot;24&quot; &quot;&quot; 13 &quot;2&quot; 1 -6 9 crd &quot;11&quot; Larry Fitzg~ &quot;37&quot; &quot;WR&quot; 13 &quot;13&quot; 0 0 10 crd &quot;85&quot; Dan Arnold &quot;25&quot; &quot;te&quot; 16 &quot;5&quot; 0 0 11 crd &quot;19&quot; KeeSean Joh~ &quot;24&quot; &quot;&quot; 8 &quot;1&quot; 0 0 12 crd &quot;81&quot; Darrell Dan~ &quot;26&quot; &quot;te&quot; 12 &quot;8&quot; 0 0 13 crd &quot;87&quot; Maxx Willia~ &quot;26&quot; &quot;TE&quot; 9 &quot;8&quot; 0 0 14 crd &quot;16&quot; Trent Sherf~ &quot;24&quot; &quot;&quot; 15 &quot;1&quot; 0 0 15 crd &quot;80&quot; Jordan Thom~ &quot;24&quot; &quot;&quot; 4 &quot;0&quot; 0 0 16 crd &quot;47&quot; Zeke Turner &quot;24&quot; &quot;&quot; 16 &quot;0&quot; 0 0 17 crd &quot;38&quot; Jonathan Wa~ &quot;23&quot; &quot;&quot; 14 &quot;0&quot; 0 0 18 crd &quot;86&quot; Seth Devalve &quot;27&quot; &quot;&quot; 4 &quot;0&quot; 0 0 19 crd &quot;&quot; Team Total &quot;27.~ &quot;&quot; 16 &quot;&quot; 479 2237 20 crd &quot;&quot; Opp Total &quot;&quot; &quot;&quot; 16 &quot;&quot; 436 2008 # ... with 19 more variables: rushing_td &lt;chr&gt;, rushing_lng &lt;chr&gt;, # rushing_y_a &lt;chr&gt;, rushing_y_g &lt;chr&gt;, rushing_a_g &lt;chr&gt;, rushing_fmb &lt;chr&gt;, # receiving_tgt &lt;chr&gt;, receiving_rec &lt;chr&gt;, receiving_yds &lt;chr&gt;, # receiving_y_r &lt;chr&gt;, receiving_td &lt;chr&gt;, receiving_lng &lt;chr&gt;, # receiving_r_g &lt;chr&gt;, receiving_y_g &lt;chr&gt;, receiving_ctch_percent &lt;chr&gt;, # receiving_y_tgt &lt;chr&gt;, yds_touch &lt;chr&gt;, yds_y_tch &lt;chr&gt;, yds_y_scm &lt;chr&gt; [[1]] # A tibble: 21 x 28 team no player age pos games_g games_gs rushing_att rushing_yds &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 atl 21 Todd Gurley 26 &quot;RB&quot; 15 15 195 678 2 atl 23 Brian Hill 25 &quot;&quot; 16 1 100 465 3 atl 25 Ito Smith 25 &quot;&quot; 14 0 63 268 4 atl 2 Matt Ryan 35 &quot;QB&quot; 16 16 29 92 5 atl 18 Calvin Ridl~ 26 &quot;WR&quot; 15 15 5 1 6 atl 40 Keith Smith 28 &quot;FB&quot; 16 7 4 7 7 atl 36 Tony Brooks~ 26 &quot;&quot; 1 0 3 4 8 atl 8 Matt Schaub 39 &quot;&quot; 1 0 3 -4 9 atl 83 Russell Gage 24 &quot;wr&quot; 16 8 2 9 10 atl 15 Brandon Pow~ 25 &quot;&quot; 15 1 2 7 # ... with 11 more rows, and 19 more variables: rushing_td &lt;chr&gt;, # rushing_lng &lt;chr&gt;, rushing_y_a &lt;chr&gt;, rushing_y_g &lt;chr&gt;, rushing_a_g &lt;chr&gt;, # rushing_fmb &lt;chr&gt;, receiving_tgt &lt;chr&gt;, receiving_rec &lt;chr&gt;, # receiving_yds &lt;chr&gt;, receiving_y_r &lt;chr&gt;, receiving_td &lt;chr&gt;, # receiving_lng &lt;chr&gt;, receiving_r_g &lt;chr&gt;, receiving_y_g &lt;chr&gt;, # receiving_ctch_percent &lt;chr&gt;, receiving_y_tgt &lt;chr&gt;, yds_touch &lt;chr&gt;, # yds_y_tch &lt;chr&gt;, yds_y_scm &lt;chr&gt; The first two elements of the resulting lists are the scraped rushing and receiving statistics for the 2020 Arizona Cardinals and the 2020 Atlanta Falcons. These data sets, along with the other thirty teams statistics, need to be cleaned in the same manner as the 2020 Denver Broncos. While we originally used the map function to iterate over a vector of web addresses, we can also use it to iterate over the list of each teams scraped statistics, applying the same cleaning procedure to each element. Rather than use a lambda expression, we define a function. Lambda expressions are a convenient and concise way to specify a transformation if the transformation is defined by a single function. Previously, the entire act of scraping was described by one function: get_team_stats(). Since the cleaning procedure consists of multiple functions such as mutate, filter, and select, it is more convenient to specify the transformation in a function, rather than a lambda expression. team_stats_clean &lt;- team_stats %&gt;% # clean map(., function(x){ x %&gt;% mutate(., age = as.numeric(age), pos = pos %&gt;% str_to_upper() %&gt;% na_if(., &#39;&#39;) %&gt;% as_factor(), receiving_ctch_percent = str_remove_all(receiving_ctch_percent, &#39;%&#39;)) %&gt;% mutate_at(., vars(starts_with(&#39;games_&#39;), starts_with(&#39;rushing_&#39;), starts_with(&#39;receiving_&#39;), starts_with(&#39;yds_&#39;)), ~as.numeric(.x)) %&gt;% mutate_all(., ~na_if(.x, &#39;&#39;)) %&gt;% filter(., !(player %in% c(&#39;Team Total&#39;, &#39;Opp Total&#39;)), !is.na(pos), pos != &#39;P&#39;) %&gt;% select_at(., vars(team, no, player, age, pos, starts_with(&#39;games_&#39;), starts_with(&#39;rushing_&#39;), starts_with(&#39;receiving_&#39;))) } ) We can always rewrite functions within a map as a lambda expression if we first define the desired transformation as a function before calling the function as a lambda expression in the map. # define function with desired transfomation clean_team_stats &lt;- function(x){ x %&gt;% mutate(., age = as.numeric(age), pos = pos %&gt;% str_to_upper() %&gt;% na_if(., &#39;&#39;) %&gt;% as_factor(), receiving_ctch_percent = str_remove_all(receiving_ctch_percent, &#39;%&#39;)) %&gt;% mutate_at(., vars(starts_with(&#39;games_&#39;), starts_with(&#39;rushing_&#39;), starts_with(&#39;receiving_&#39;), starts_with(&#39;yds_&#39;)), ~as.numeric(.x)) %&gt;% mutate_all(., ~na_if(.x, &#39;&#39;)) %&gt;% filter(., !(player %in% c(&#39;Team Total&#39;, &#39;Opp Total&#39;)), !is.na(pos), pos != &#39;P&#39;) %&gt;% select_at(., vars(team, no, player, age, pos, starts_with(&#39;games_&#39;), starts_with(&#39;rushing_&#39;), starts_with(&#39;receiving_&#39;))) } # map desired transformation to each team&#39;s scraped statistics using lambda # expression team_stats_clean_lambda &lt;- team_stats %&gt;% map(., ~clean_team_stats(.x)) # show results using funciton in map and lambda expression in map are the same identical(team_stats_clean, team_stats_clean_lambda) [1] TRUE After mapping the cleaning procedure to each of the teams scraped statistics, the result is a list where each element of the list is a tidy data frame consisting of a teams rushing and receiving statistics. As discussed in Chapter 2, we can rectangularize this list of data frames by binding the rows of each of these data frames, creating a column to indicate which team each row belongs. team_stats_clean %&gt;% bind_rows(.) Since the purrr package is a member of the Tidyverse, it is created with principles of Tidy data in mind. Transforming a list to a vector, or in our case, a data frame is often a common last step after mapping a function. To accommodate this need, the purrr package provides specific maps such as map_dbl and map_chr which transform the resulting list into a vector of type double or character, respectively. There is also map_dfr and map_dfc which binds the list elements together row-wise or column-wise, respectively, to return a data frame. If the result cannot be transformed into the requested type, an error will be returned. The mapping illustrated in this example can be written concisely as rush_receive &lt;- team_urls %&gt;% map(., ~get_team_stats(.x)) %&gt;% map_dfr(., ~clean_team_stats(.x)) "],["parallelization-with-furrr.html", "3.3 Parallelization with furrr", " 3.3 Parallelization with furrr When we map the function to scrape team statistics over the vector of team web addresses, we are insinuating that the function within the map (e.g. get_team_stats) should be applied to each element of the vector (e.g. team_urls) sequentially. Computational tasks which involve many separate, independently executable jobs should be run in parallel. When jobs are run in parallel, this means they are run at the same time, rather than sequentially. If the computational burden at each step is larger than the computational burden of setting up instructions for parallelization, then running code in parallel will save time. While there are different types of parallelization, we will only focus on one: multi-core parallelization, which allows us to make use of the whole computer rather than rely on single processor architecture. furrr, a new Tidyverse package, attempts to make mapping in parallel easy and pain-free by combining the functionality of the purrr package and the future package. By combining purrrs mapping capabilities and futures parallel processing capabilities, furrr allows for parallelization with similar syntax. In the previous subsection, we learned how to apply mapping functions to scrape and clean the statistics for each team, sequentially. Using furrr, we can parallelize this process. We will juxtapose each approach for comparison. # sequential computation using purrr rush_receive &lt;- team_urls %&gt;% map(., ~get_team_stats(.x)) %&gt;% map_dfr(., ~clean_team_stats(.x)) # parallel computation using furrr library(furrr) future::plan(multiprocess) rush_receive_parallel &lt;- team_urls %&gt;% future_map(., ~get_team_stats(.x)) %&gt;% future_map_dfr(., ~clean_team_stats(.x)) # compare output identical(rush_receive, rush_receive_parallel) [1] TRUE Lets compare the speed of the operations. # sequentially system.time( rush_receive &lt;- team_urls %&gt;% map(., ~get_team_stats(.x)) %&gt;% map_dfr(., ~clean_team_stats(.x)) ) # parallel system.time( rush_receive_parallel &lt;- team_urls %&gt;% future_map(., ~get_team_stats(.x)) %&gt;% future_map_dfr(., ~clean_team_stats(.x)) ) user system elapsed 2.65 0.09 3.65 user system elapsed 0.28 0.03 1.09 We can see that parallelizing this process is about one and a half times faster than applying these functions sequentially. This effect is only amplified when we increase the number of sources to scrape. player_urls &lt;- team_urls %&gt;% map(., ~get_players(.x)) %&gt;% flatten_chr() # sequentially system.time( player_stats &lt;- player_urls %&gt;% map(., ~get_player_stats(.x)) ) # parallel system.time( player_stats_par &lt;- player_urls %&gt;% future_map(., ~get_player_stats(.x)) ) user system elapsed 35.20 1.63 153.31 user system elapsed 0.16 0.01 9.42 As we can see, parallelizing the scraping of all 627 players who recorded rushing and receiving statistics in 2020 is about five times faster than scraping the same data sequentially. "],["scratch-for-iterative-processes.html", "3.4 Scratch for Iterative Processes", " 3.4 Scratch for Iterative Processes In this case, the map function is sufficient since the scraped results for the Denver Broncos does not depend on the scraped results for the Dallas Cowboys. If the results of one iteration (e.g. the scraped results for the Denver Broncos) depended on the results of prior iterations (e.g. scraped results for the Dallas Cowboys), then the algorithm to scrape team statistics is recursive. When we have a recursive algorithm, the map function is often insufficient or overly complex. In this case, we may reach for a for-loop using the for function. A for-loop allows us to repeat code multiple times making small changes with each iteration. To illustrate the concept of a for-loop, lets consider a famous example of a recursive algorithm: the Fibonacci sequence. The Fibonacci sequence is a sequence of integers starting with 0 and 1 where each following integer is the sum of the previous two integers. Written out, the first eight values of the Fibonacci sequence are \\(0, 1, 1, 2, 3, 5, 8, 13\\). Lets consider how to use a for-loop to return the first \\(n\\) integers in the Fibonacci sequence. Lets consider \\(n=8\\) to start. n &lt;- 8 fib_seq &lt;- vector(mode = &#39;integer&#39;, length = n) for(i in 1:n){ # if it is the first element of the Fibonacci sequence, return a 0 if(i == 1){ fib_seq[i] &lt;- 0 } # if it is the second element of the Fibonacci sequence, return a 1 else if(i == 2){ fib_seq[i] &lt;- 1 } # if it is any other element of the Fibonacci sequence, return the sum of the # previous two elements else{ fib_seq[i] &lt;- fib_seq[i-1] + fib_seq[i-2] } } fib_seq [1] 0 1 1 2 3 5 8 13 Since each value of the Fibonacci sequence depends on the results of the prior two values of the Fibonacci sequence, a for-loop allows us to index the results of the sequence with each iteration. While intuitive, the for-loops can be very slow in R, especially when the recursive algorithm becomes more complex. In these situations, we need a faster alternative. "],["Visualization.html", "Chapter 4 Visualization", " Chapter 4 Visualization Mankind invented a system to cope with the fact that we are so intrinsically lousy at manipulating numbers. Its called the graph. Charlie Munger, Vice Chairman, Berkshire Hathaway In Chapter 3, we discussed tools for cleaning data. In this chapter, we will turn our attention to the visualization of cleaned data. There are several approaches to creating graphics in R. Here, we focus on three: base, ggplot, and plotly. In this chapter, we hope to outline the syntax of these systems before providing some examples using the data prepared in previous chapters. "],["base.html", "4.1 base", " 4.1 base Before the development of ggplot and the grammer of graphics (discussed in the next subsection), R users largely relied on base to construct data visualization. base R provide flexible but verbose solutions to construct any type of graph. At the heart of visualizations with base is the generic function plot which is used for plotting R objects. Most simply, plot is used to construct a scatter plot. It takes either a formula (e.g. plot(y~x, data = dt)), specifying that we wish to plot column y on the vertical axis and x on the horizontal axis, or specifications for the coordinates of x and y (e.g. plot(x = dt$x, y = dt$y)). rush_receive &lt;- filter(rush_receive, pos %in% c(&#39;QB&#39;, &#39;RB&#39;, &#39;WR&#39;, &#39;TE&#39;)) # formula plot(receiving_lng~receiving_ctch_percent, data = rush_receive) # coordinates plot(x = rush_receive$receiving_ctch_percent, y = rush_receive$receiving_lng) Notice, the plots are fundamentally the same. The only differences are the axis labels. These can be adjusted with plotting parameters. Rather than outline all of the parameters, we refer you to the following figure provided by Holtz (2018), software engineer for Datadog. His website, The R Graph Gallery, is an excellent resource for those who wish to dig deeper into data visualizations. Figure 4.1: common plotting parameters for base R plots Lets use some of these parameters to clean up the previous scatterplot. Well create a title, label the axes, change the character used for the points, and color the points based on the position of the player. plot(receiving_lng~receiving_ctch_percent, data = rush_receive, main = &#39;Longest Catch vs Catch Percentage&#39;, # title xlab = &#39;Catch Percentage (%)&#39;, # x-axis label ylab = &#39;Longest Catch (yds)&#39;, # y-axis label pch = 20, # point type col = pos) # point color If we color the points by position, then we need to provide our readers with a legend illustrating what each color represents. As previously mentioned, base R is incredibly flexible, so tweaking a plot (such as adding a legend) is almost always possible. However, doing so can be a real headache. To illustrate the verbosity of base, we will illustrate how to add a legend to the previous plot. There are generally two ways people proceed, both of which are drastically more involved and cumbersome than that of ggplot. For this reason, among others, we prefer ggplot to base visualizations. It is of the authors humble opinion that aesthetic attributes such as color, shape, fill, among others should never be added to a plot unless it provides further information about the data of interest. That is, do not add flare for flares sake. Every element of a plot should portray some bit of information. The first method is to partition the data by position and add the positions points iteratively to the scatterplot, specifying a new color for each of them. Essentially, we construct the full plot with black points before overwriting the points, one position at a time, with colored points. While easy to comprehend, this method is incredibly verbose and involves duplicate code. plot(receiving_lng~receiving_ctch_percent, data = rush_receive, main = &#39;Longest Catch vs Catch Percentage&#39;, # title xlab = &#39;Catch Percentage (%)&#39;, # x-axis label ylab = &#39;Longest Catch (yds)&#39;, # y-axis label pch = 20) # point character points(x = rush_receive[rush_receive$pos == &#39;QB&#39;, ]$receiving_ctch_percent, y = rush_receive[rush_receive$pos == &#39;QB&#39;, ]$receiving_lng, pch = 20, col = &#39;black&#39;) points(x = rush_receive[rush_receive$pos == &#39;RB&#39;, ]$receiving_ctch_percent, y = rush_receive[rush_receive$pos == &#39;RB&#39;, ]$receiving_lng, pch = 20, col = &#39;gold&#39;) points(x = rush_receive[rush_receive$pos == &#39;WR&#39;, ]$receiving_ctch_percent, y = rush_receive[rush_receive$pos == &#39;WR&#39;, ]$receiving_lng, pch = 20, col = &#39;blue&#39;) points(x = rush_receive[rush_receive$pos == &#39;TE&#39;, ]$receiving_ctch_percent, y = rush_receive[rush_receive$pos == &#39;TE&#39;, ]$receiving_lng, pch = 20, col = &#39;green&#39;) legend(&quot;topleft&quot;, legend = c(&#39;QB&#39;, &#39;RB&#39;, &#39;WR&#39;, &#39;TE&#39;), col = c(&#39;black&#39;, &#39;gold&#39;, &#39;blue&#39;, &#39;green&#39;), pch = 20, pt.cex = 1, cex = 0.6, text.col = &quot;black&quot;, horiz = F, inset = c(0.05, 0.05)) The second method is to replace the vector of positions with a vector of colors. This can be done by matching the position to a respective color. It is important to ensure that the order of the colors used in the match function is the same order specifed in the legend; otherwise, you may be misleading your reader by falsely claiming, for example, that black points represent wide receivers when in fact they represent quarterbacks. While less verbose, this method is prone to transcription mistakes in this way. plot(receiving_lng~receiving_ctch_percent, data = rush_receive, main = &#39;Longest Catch vs Catch Percentage&#39;, # title xlab = &#39;Catch Percentage (%)&#39;, # x-axis label ylab = &#39;Longest Catch (yds)&#39;, # y-axis label pch = 20, # point type col = c(&#39;black&#39;, &#39;gold&#39;, &#39;blue&#39;, &#39;green&#39;)[match(rush_receive$pos, c(&#39;QB&#39;, &#39;RB&#39;, &#39;WR&#39;, &#39;TE&#39;))]) # point color legend(&quot;topleft&quot;, legend = c(&#39;QB&#39;, &#39;RB&#39;, &#39;WR&#39;, &#39;TE&#39;), col = c(&#39;black&#39;, &#39;gold&#39;, &#39;blue&#39;, &#39;green&#39;), pch = 20, pt.cex = 1, cex = 0.6, text.col = &quot;black&quot;, horiz = F, inset = c(0.05, 0.05)) While we outlined the common base plotting parameters using a scatterplot, they also apply to a vast array of other base plots such as bar plots, line plots, histograms, etc. To read more about base plots, see this helpful link. References "],["ggplot.html", "4.2 ggplot", " 4.2 ggplot In the previous subsection, we outlined how to construct plots in base R. While flexible, base plots are essentially a series of R commands, lacking a universal graphical language which makes it difficult to translate from one plot to another. To fill this void, ggplot was created, built from concepts introduced in Grammar of Graphics in Wilkinson (2012). In essence, the grammer of graphics asserts that every graph is composed of data in a coordinate system where the data are represented by geometric objects (such as points, lines, and bars) with aesthetic attributes (such as color, shape, and size). Furthermore, a graph may contain statistical transformations of the data. These are the ingredients of ggplot, outlined in the following Figure taken from the ggplot cheat sheeet. Figure 4.2: visualization of the grammer of graphics To exemplify the process, lets create the same scatterplot from the previous subsection. The ggplot function is used to specify the desired map. As arguments, it requires the data set (rush_receive) and the aesthetics. Here, three aesthetics are specified: the variable used for the x-coordinates (receiving_ctch_percent), the variable used for the y-coordinates (receiving_lng), and the color (pos). The color aesthetic assigns a color for each unique value of position. Since there are four positions in consideration, there will be four colors used. Once the data set and aesthetics are specified, we need to specify how to visually represent the data with geometries. Here, we wish to create a scatterplot, so we will use points to represent the data (i.e. geom_point()). Without specifying the coordinate system, the Cartesian coordinate system generated by the x- and y-axis variables is used by default. p &lt;- ggplot(data = rush_receive, aes(x = receiving_ctch_percent, y = receiving_lng, color = pos)) + geom_point() p Once the basics of the plot are created, we can then fine tune the scales, labels, and theme. One quick and color-blind friendly option for setting the colors used is with scale_color_brewer which uses color palettes from the RColorBrewer package. Furthermore, the name and breaks of the x- and y-axis can be changed with their respective scales. Since both axes represent continuous quantities we use scale_x_continuous and scale_y_continuous. Titles, subtitles, and captions can be changed with labs. The overall appearance of the graph can be changed with premade themes such as theme_bw or you can create your own using theme. Here, we use theme to move the legend to the bottom of the graph. p &lt;- p + scale_color_brewer(&#39;Position&#39;, palette = &#39;Set1&#39;) + scale_x_continuous(name = &#39;Catch Percentage (%)&#39;) + scale_y_continuous(name = &#39;Longest Catch (yds)&#39;) + labs(title = &#39;Longest Catch vs Catch Percentage&#39;) + theme_bw() + theme(legend.position = &#39;bottom&#39;) p After a graph is created, you can add elements to the graph through layers. Layers can use the same aesthetics as the orignal graph, such as adding a scatterplot smoother for each position: p &lt;- p + geom_smooth(se = FALSE) Or, layers can be defined from different data sets and aesthetics, such as labeling running backs in the NFL who attended the University of Georgia: uga_rbs &lt;- filter(rush_receive, player %in% c(&quot;Todd Gurley&quot;, &quot;Nick Chubb&quot;, &quot;Sony Michel&quot;, &quot;D&#39;Andre Swift&quot;)) p &lt;- p + geom_label(data = uga_rbs, aes(x = receiving_ctch_percent, y = receiving_lng, label = player), size = 2, inherit.aes = FALSE) p + labs(caption = &#39;Go Dawgs!&#39;) A popular and incredibly useful layer in ggplot is a facet. Facets allow the user to create the same plots for every value of a variable. For example, in the previous scatterplot, we colored the points by the players position. Instead, if we wanted a seperate scatterplot for each position, we could apply a facet. p + facet_wrap(~pos) + theme(legend.position = &#39;none&#39;) References "],["plotly.html", "4.3 plotly", " 4.3 plotly Previously, we created labels for a handful of players names. If we wanted to create labels for every players name, then our plot would quickly become overwhelming and all information contained in the graph would be lost. Furthermore, static labels can only provide information for one variable, such as the players name. Imagine a general manager are presented these data and wishes to use them to make a trade. They may wish to procure a running back which has demonstrated his ability to catch the ball. There may be several data points of interest, but for each of them, the manager would need further information regarding each of the players such as his name, team, and perhaps age. We might not want to overwhelm the manager with all this information but allow them to easily retrieve it at will. In this case, we would like to make an interactive graph where information is embedded in the points. To create such a graph, we can utilize the plotly package. We will create a ggplot with a sequence of labels for information wed like to be accessed interactively. In this case, we will create the same faceted scatterplot as before with labels for the players team, name, and age. After we create the plot of interest, we can use the function ggplotly to create an interactive widget. This allows the user to pan the graph, zoom in on areas of interest, and hover over data points to retrieve more information about the observation. While this simply scratches the surface of plotly, it covers the most widely needed functionality. For more information regarding plotly, follow this link. # create ggplot with labels p &lt;- ggplot(data = rush_receive, aes(x = receiving_ctch_percent, y = receiving_lng, color = pos, label = team, label2 = player, label3 = age)) + geom_point() + scale_color_brewer(&#39;Position&#39;, palette = &#39;Set1&#39;) + scale_x_continuous(name = &#39;Catch Percentage (%)&#39;) + scale_y_continuous(name = &#39;Longest Catch (yds)&#39;) + labs(title = &#39;Longest Catch vs Catch Percentage&#39;) + theme_bw() + theme(legend.position = &#39;none&#39;) + facet_wrap(~pos) # create interactive plot, indicating which labels should be accessed interactively # install.packages(&#39;plotly&#39;) library(plotly) ggplotly(p, tooltip = c(&#39;team&#39;, &#39;player&#39;, &#39;age&#39;)) "],["supplemental.html", "Chapter 5 Supplemental", " Chapter 5 Supplemental You can have data without information, but you cannot have information without data.  Daniel Keys Moran, Computer Scientist and Author "],["scraping-in-the-wild.html", "5.1 Scraping in the Wild", " 5.1 Scraping in the Wild In Chapter 2, we introduced the idea of rectangular data vs. non-rectangular data, providing examples for each and demonstrating the process of rectangularization. We outlined how to use a web-API before introducing the concept of web scraping by illustrating the language of the web: HTML. Since webpages can be complicated, scraping can be complicated. In this chapter, we will leverage the Selector Gadget and our knowledge of HTML elements to scrape data from various sources. It is our belief that the only way to teach web scraping is through examples. Each example will become slightly more difficult than the previous. 5.1.1 Lessons Learned from Scraping Scraping is a necessary evil that requires patience. While some tasks may prove easy, you will quickly find others seem insurmountable. In this section, we will outline a few tips to help you become a web scraper. Brainstorm! Before jumping into your scraping project, ask yourself what data do I need and where can I find it? If you discover you need data from various sources, what is the unique identifier, the link which ties these data together? Taking the time to explore different websites can save you a vast amount of time in the long run. As a general rule, simplistic looking websites are generally easier to scrape and often contain the same information as more complicated websites with several bells and whistles. Start small! Sometimes a scraping task can feel daunting, but it is important to view your project as a war, splitting it up into small battles. If you are interested in the racial demographics of each of the United States, consider how you can first scrape this information for one state. In this process, dont forget tip 1! Hyperlinks are your friend! They can lead to websites with more detailed information or serve as the unique identifier you need between different data sources. Sometimes you wont even need to scrape the hyperlinks to navigate between webpages, making minor adjustments to the web address will sometimes do. Data is everywhere! Text color, font, or highlighting may serve as valuable data that you need. If these features exist on the webpage, then they exist within the HTML code which generated the document. Sometimes these features are well hidden or even inaccessible, leading to the last and final tip. Ready your search engine! Just like coding in R is an art, web developing is an art. When asking distinct developers to create the same website with the same functionality, the final result may be similar but the underlying HTML code could be drastically different. Why does this matter? You will run into an issue that hasnt been addressed in this text. Thankfully, if youve run into an issue, someone else probably has too. We cannot recommend websites like Stack Overflow enough. 5.1.2 Scraping NFL Data In Chapter 2, we gathered some betting data pertaining to the NFL through a web-API. We may wish to supplement these betting data with data pertaining to NFL teams, players, or even playing conditions. As we progress through this sub-section, examples will become increasingly problematic or troublesome. The goal in this subsection is to introduce you to scraping by heeding the advice given in the Chapter 5.1.1. Following our own advice, lets brainstorm. When you think of NFL data, you probably think of NFL.com or ESPN. After further digging, we will explore Pro Football Reference, a reliable archive for football statistics (with a reasonably simple webpage). This is an exhaustive source which boasts team statistics, player statistics, and playing conditions for various seasons. Lets now start small by focusing on team statistics, but further, lets limit our scope to the 2020 Denver Broncos. Notice, there are hyperlinks for each player documented in any of the categories, as well hyperlinks for each games boxscore where there is information about playing conditions and outcomes. Hence, we have a common thread between team statistics, players, and boxscores. If, for example, we chose to scrape team statistics from one website and player statistics from another website, we may have to worry about a unique identifier (being team) if the websites have different naming conventions. 5.1.2.1 HTML Tables: Team Statistics Well start with the team statistics for the 2020 Denver Broncos which can be found in a table entitled Team Stats and Rankings. Well need to figure in which element or node the table lives within the underlying HTML. To do this, we will utilize the CSS selector gadget. If we highlight over and click the table with the selector gadget, we will see that the desired table lives in an element called #team_stats. Figure 5.1: finding the team statistics element using the selector gadget Alternatively, we could view the page source and search for the table name. Ive highlighted the information identified by the selector gadget with the cursor. Figure 5.2: finding the team statistics element using the page source While the selector gadget is always a great first option, it is not always reliable. There are instances when the selector gadget identifies a node that is hidden or inaccessible without JavaScript. In these situations, it is best view the page source directly for more guidance on how to proceed. Practice with both the selector gadget and the page source. Once we have found the name of the element containing the desired data, we can utilize the rvest package to scrape the table. The general process for scraping an HTML table is Read the HTML identified by the web address. Isolate the node containing the data we desire. Parse the HTML table. Take a look at the data to ensure the columns are appropriate labels. library(rvest) library(janitor) pfr_url &lt;- &quot;https://www.pro-football-reference.com&quot; broncos_url &lt;- str_c(pfr_url, &#39;/teams/den/2020.htm&#39;) broncos_url %&gt;% # read the HTML read_html(.) %&gt;% # isolate the node containing the HTML table html_node(., css = &#39;#team_conversions&#39;) %&gt;% # parse the html table html_table(.) %&gt;% # make the first row of the table column headers and clean up column names row_to_names(., row_number = 1) %&gt;% clean_names() While these data need cleaning up before they can be used in practice, we will defer these responsibilities to Chapter 3. In the next subsection, we will take a deeper dive into scraping HTML tables using information attained from attribute values, a common occurrence in web scraping. Take this time to scrape the Team Conversions table on your own. Any feedback for this section? Click here 5.1.2.2 Commented and Hidden HTML Tables: Player Statistics Lets transition to gathering player statistics, particularly for those players who have recorded statistics in the rushing or receiving category. These data are given in the table entitled Rushing and Receiving. Lets use the selector gadget to identify the node containing the table and scrape the table according to the previous section. broncos_url %&gt;% # read the HTML read_html(.) %&gt;% # isolate the node containing the HTML table html_node(., css = &#39;#rushing_and_receiving&#39;) %&gt;% # parse the html table html_table(.) Error in UseMethod(\"html_table\"): no applicable method for 'html_table' applied to an object of class \"xml_missing\" We get an error stating that the node #rushing_and_receiving does not contain an HTML table. In fact, there doesnt appear to be anything in that node at all. broncos_url %&gt;% # read the HTML read_html(.) %&gt;% # isolate the node containing the HTML table html_node(., css = &#39;#rushing_and_receiving&#39;) {xml_missing} &lt;NA&gt; This, of course, is contrary to the selector gadget declaring a node #rushing_and_receiving as the one containing the table we desire and directly see on the webpage. Generally when this happens, it means the node containing the information in the table has either been commented out or hidden. A node is commented out when it is contained in a comment tag: &lt;!-- --&gt;. For example, if we were to comment out the paragraph element in Chapter 2, it would look like this: &lt;!-- &lt;p class = \"fancy\"&gt; This is the paragraph you want to scrape which has been formatted in a fancy script. &lt;/p&gt; --&gt;. When a node is commented out, it exists in the HTML, but it cannot be accessed until we step into the comment tag. Otherwise said, we cant bypass the comment node. If you would like to scrape information contained with a comment tag, the general strategy is Read the HTML identified by the web address. Isolate the node containing the comment tag. Inside the comment tag are the data we desire. Isolate the comment tag. Convert the content to HTML. Isolate the node containing the table. Parse the HTML table. Take a look at the data to ensure the columns are appropriate labels. broncos_url %&gt;% read_html() %&gt;% html_node(., css = &#39;#all_rushing_and_receiving&#39;) %&gt;% html_nodes(., xpath = &#39;comment()&#39;) %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(., &#39;table&#39;) %&gt;% html_table() %&gt;% set_names(., str_c(names(.), .[1,], sep = &#39;_&#39;)) %&gt;% clean_names() %&gt;% slice(., -1) 5.1.2.3 Attributes: Player Statistics by Game Suppose now that we wish to scrape the same player statistics, but rather than get the aggregated total, we want the players statistics by game. Within the Rushing and Receiving table scraped in the previous example, there are hyperlinks for each player. When you click on this hyperlink, you will find the players statistics for each game played. These are the data we desire. We want rushing and receiving statistics for each player on each team for a variety of years. Using the lessons learned in Chapter 5.1.1, we have brainstormed the problem and have a road map to the solution. We will first scrape the hyperlink attribute to get the web address associated with each player for a given team. Within each of these web addresses, we will scrape the game by game statistics before iterating over each team in the NFL. After this, we can repeat the procedure for a variety of years. This is, of course, a big job, so lets scale down the problem to one player: Melvin Gordon III, running back. To do this, we will: Get the web addresses associated with each player in the Rushing and Receiving table, before isolating the address associated with Melvin Gordons rushing and receiving statistics. Read the HTML identified by the web address. Isolate the node containing the data we desire. Parse the HTML table. Take a look at the data to ensure the columns are appropriate labels. Lets figure out where the hyperlinks exist within the HTML code. To do this, I will look at the page source and search for Melvin Gordon. We find that the web address corresponding to his game by game player statistics exist in the hidden HTML table that we scraped in the previous example. In particular, they are the href attribute to a node entitled a which is embedded within the hidden HTML table. Phew. Lets consider how we can scrape these web addresses step by step: 1.1 Isolate the hidden HTML table, similar to the previous example. 1.2 Isolate nodes with tag a. 1.3 Extract the href attribute from these nodes. Steps 1.1 and 1.2 should be familiar from previous examples, but step 1.3 requires a new function within rvest called html_attr. Lets see how to do this in R. Figure 5.3: finding the element containing the web address cooresponding to Melvin Gordon III game by game player statistics using the page source player_urls &lt;- broncos_url %&gt;% read_html() %&gt;% html_node(., css = &#39;#all_rushing_and_receiving&#39;) %&gt;% html_nodes(., xpath = &#39;comment()&#39;) %&gt;% html_text() %&gt;% read_html() %&gt;% html_nodes(., &#39;table&#39;) %&gt;% html_nodes(., &#39;a&#39;) %&gt;% html_attr(., &#39;href&#39;) %&gt;% str_c(pfr_url, .) player_urls [1] &quot;https://www.pro-football-reference.com/players/G/GordMe00.htm&quot; [2] &quot;https://www.pro-football-reference.com/players/L/LindPh00.htm&quot; [3] &quot;https://www.pro-football-reference.com/players/L/LockDr00.htm&quot; [4] &quot;https://www.pro-football-reference.com/players/F/FreeRo00.htm&quot; [5] &quot;https://www.pro-football-reference.com/players/H/HamlKJ00.htm&quot; [6] &quot;https://www.pro-football-reference.com/players/D/DrisJe00.htm&quot; [7] &quot;https://www.pro-football-reference.com/players/R/RypiBr00.htm&quot; [8] &quot;https://www.pro-football-reference.com/players/B/BellLe02.htm&quot; [9] &quot;https://www.pro-football-reference.com/players/S/SpenDi00.htm&quot; [10] &quot;https://www.pro-football-reference.com/players/H/HintKe00.htm&quot; [11] &quot;https://www.pro-football-reference.com/players/M/MartSa01.htm&quot; [12] &quot;https://www.pro-football-reference.com/players/F/FantNo00.htm&quot; [13] &quot;https://www.pro-football-reference.com/players/J/JeudJe00.htm&quot; [14] &quot;https://www.pro-football-reference.com/players/P/PatrTi00.htm&quot; [15] &quot;https://www.pro-football-reference.com/players/H/HamiDa01.htm&quot; [16] &quot;https://www.pro-football-reference.com/players/V/VannNi00.htm&quot; [17] &quot;https://www.pro-football-reference.com/players/O/OkwuAl00.htm&quot; [18] &quot;https://www.pro-football-reference.com/players/F/FumaTr00.htm&quot; [19] &quot;https://www.pro-football-reference.com/players/C/ClevTy00.htm&quot; [20] &quot;https://www.pro-football-reference.com/players/S/SuttCo00.htm&quot; [21] &quot;https://www.pro-football-reference.com/players/B/ButtJa00.htm&quot; Since the web addresses are given as an extension to the Pro Football Reference homepage, we will need to concatenate the strings with str_c() before saving the result for future use. Now, lets continue to step two in our excursion to get Melvin Gordons game by game statistics. player_urls[1] %&gt;% read_html() %&gt;% html_node(., css = &#39;#all_stats&#39;) %&gt;% html_node(., &#39;table&#39;) %&gt;% html_table(., fill = TRUE) %&gt;% set_names(., str_c(names(.), .[1,], sep = &#39;_&#39;)) %&gt;% clean_names() %&gt;% slice(., -1) Any feedback for this section? Click here 5.1.3 Putting It All Together Lets consider how we can gather data regarding racial demographics across each state in the United States of America. Perusing Wikipedia, notice that each state has a table documenting the states racial population breakdown. Figure 5.4: part of brainstorming is exploring different websites which may have the data you desire If we were to go to Georgia, we would notice that the web address is similar to Alabama, but since Georgia is also a country, the web address specifies that you are currently on the webpage for the state rather than the country. This effectively rules out editing the web address to iterate over each state. If we broaden our search to US States, we find a Wikipedia page with a list of each state. Each state has a hyperlink which takes us to the states webpage with the demographic information we desire. Remember: hyperlinks are our friend! If they exist, we can scrape them, iterating over the web addresses to get data from each state. Lets use what we just learned to attain the web addresses for each state. Well start with the web address containing a list of every state. wiki_url &lt;- &quot;https://www.wikipedia.org&quot; states_url &lt;- str_c(wiki_url, &#39;/wiki/U.S._state#States_of_the_United_States&#39;) Lets use the page source to find where the hyperlinks exist in the HTML code. To do this, I am going to search for the phrase The 50 U.S. states, in alphabetical order, along with each states flag in the page source since the list of states is just below this phrase. Figure 5.5: finding the element containing the list of states with their cooresponding web addresses using the page source We find that the list of states, among other information, exists in an element tagged div with the attribute class set to an attribute value of plainlist. Within this node, the list of states exists in an element tagged ul and each state exists in an element tagged li. Within each state, the web address corresponding to that states individual Wikipedia article is given in the href attribute to an element tagged a. Lets use this information to extract the web addresses corresponding to each state. We may also want to extract the state name which is the content of the element tagged a. node_w_states &lt;- states_url %&gt;% read_html() %&gt;% html_nodes(xpath = &quot;//div[@class=&#39;plainlist&#39;]&quot;) %&gt;% html_nodes(&#39;ul&#39;) %&gt;% html_nodes(&#39;li&#39;) %&gt;% html_nodes(&#39;a&#39;) ind_states &lt;- tibble(state = node_w_states %&gt;% html_text(), url = node_w_states %&gt;% html_attr(., &#39;href&#39;) %&gt;% str_c(wiki_url, .)) ind_states Lets consider how we would scrape the racial demographics. Use either the selector gadget or the page source to identify where the desired table exists in the HTML code. ind_states$url[1] %&gt;% read_html() %&gt;% html_nodes(., xpath = &quot;//table[@class=&#39;wikitable sortable collapsible&#39;]&quot;) %&gt;% html_table() %&gt;% as.data.frame() In the next chapter, we will learn the tools to scrape the racial demographics for each state using the vector of web addresses we attained through this process as well as cleaning up these data according to tidy principles. Any feedback for this section? Click here "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
