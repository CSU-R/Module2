[
["index.html", "R Module 2 Chapter 1 Welcome!", " R Module 2 Connor Gibbs1 14 Oct, 2020, 12:52 PM Chapter 1 Welcome! Hi, and welcome to the R Module 2 (AKA STAT 158) course at Colorado State University! This course is the second of three 1 credit courses intended to introduce the R programming language, specifically the Tidyverse. Through these Modules (courses), we’ll explore how R can be used to do the following: Access data via files or web application programming interfaces (APIs) Scrape data from web Wrangle and clean complicated data structures Create graphics with an eye for quality and aesthetics Understand data using basic modeling In addition, you’ll also be exposed to broader concepts, including: Data organization and storage Hypertext Markup Language (HTML) Tidyverse principles More detail will be provided in the Course Topics laid out in the next chapter. 1.0.1 How To Navigate This Book To move quickly to different portions of the book, click on the appropriate chapter or section in the the table of contents on the left. The buttons at the top of the page allow you to show/hide the table of contents, search the book, change font settings, download a pdf or ebook copy of this book, or get hints on various sections of the book. The faint left and right arrows at the sides of each page (or bottom of the page if it’s narrow enough) allow you to step to the next/previous section. Here’s what they look like: Figure 1.1: Left and right navigation arrows Department of Statistics, Colorado State University, connor.gibbs@colostate.edu↩ "],
["associated-csu-course.html", "1.1 Associated CSU Course", " 1.1 Associated CSU Course This bookdown book is intended to accompany the associated course at Colorado State University, but the curriculum is free for anyone to access and use. If you’re reading the PDF or EPUB version of this book, you can find the “live” version at https://csu-r.github.io/Module2/, and all of the source files for this book can be found at https://github.com/CSU-R/Module2. If you’re not taking the CSU course, you will periodically encounter instructions and references which are not relevant to you. For example, we will make reference to the Canvas website, which only CSU students enrolled in the course have access to. "],
["AccessingData.html", "Chapter 2 Accessing Data", " Chapter 2 Accessing Data “Data is the new oil.” —Clive Humby, Chief Data Scientist, Starcount In this chapter, we’ll cover how to access data given in various forms and provided from various sources. "],
["rectangular-vs-non-rectangular-data.html", "2.1 Rectangular vs. Non-rectangular Data", " 2.1 Rectangular vs. Non-rectangular Data Data present themselves in many forms, but at a basic level, all data can be categorized into two structures: rectangular data and non-rectangular data. Intuitively, rectangular data are shaped like a rectangle where every value corresponds to some row and column. Non-rectangular data, on the other hand, are not no neatly arranged in rows and columns. Instead, they are often a culmination of separate data structures where there is some similarity among members of the same data structure. To motivate this idea, let’s consider a basic grocery list which consists of ten items: black beans, milk, pasta, cheese, bananas, peanut butter, bread, apples, tomato sauce, and mayonnaise. Notice, there is little organization to this list, and more involved shoppers may find this list inadequate or unhelpful. We may wish to group these items by sections in which we’re likely to find them. We may also want to include prices, so we know in-store whether the items are on sale. Let’s consider two distinct (but legitimate) ways to organize these data. To illustrate the idea of rectangular vs. non-rectangular data, we will consider how these data can be structured in both ways using R. You may not have seen some of these functions yet. No worries! The objective is not to understand how to utilize these functions but to comprehend the difference between rectangular and non-rectangular data. One may first consider grouping these items by section. For example, apples and bananas can be found in the produce section, whereas black beans and tomato sauce can be found in the canned goods. If we were to continue to group these items by section, we may arrive at a data set which looks something like this: groc &lt;- list(produce = data.frame(item = c(&#39;apples&#39;, &#39;bananas&#39;), price = c(3.99, 0.49)), condiments = data.frame(item = c(&#39;peanut_butter&#39;, &#39;mayonnaise&#39;), price = c(2.18, 3.89)), canned_goods = data.frame(item = c(&#39;black_beans&#39;, &#39;tomato_sauce&#39;), price = c(0.99, 0.69)), grains = data.frame(item = c(&#39;bread&#39;, &#39;pasta&#39;), price = c(2.99, 1.99)), dairy = data.frame(item = c(&#39;milk&#39;, &#39;butter&#39;), price = c(2.73, 2.57))) groc $produce item price 1 apples 3.99 2 bananas 0.49 $condiments item price 1 peanut_butter 2.18 2 mayonnaise 3.89 $canned_goods item price 1 black_beans 0.99 2 tomato_sauce 0.69 $grains item price 1 bread 2.99 2 pasta 1.99 $dairy item price 1 milk 2.73 2 butter 2.57 Here, we use lists and data frames to create a data set of our grocery list. This list can be traversed depending on what section of the store we find ourselves. For example, suppose we are in the produce section, and we need to recall what items to buy. We could utilize the following code to remind ourselves. groc$produce Is this grocery list an example of rectangular or non-rectangular data? Are there examples of rectangular data contained within the grocery list? How could we restructure the data to rectangularize the grocery list? As constructed, this grocery list is an example of non-rectangular data. As a whole, the grocery list is not shaped like a rectangle, but rather, consists of sets of rectangular data, where the sets are defined by the section of the store. Within a section of the store, the items and prices are given in rectangular form since every value is defined by a row and column. While non-rectangular data is often a useful return object for user-defined functions, they are often troublesome to work with. If a data set can be restructured or created in rectangular form, it should be. Rectangular data is especially important within the Tidyverse, a self-described ‘opinionated collection of R packages designed for data science’. All packages within the Tidyverse rely on the principle of tidy data, data structures where observations are given by rows and variables are given by columns. As defined, tidy data are rectangular, so as we embark on wrangling, visualizing, and modeling data in future chapters, it is important to ponder the nature of our data and whether it can be rectangularized. Let’s consider how we can rectangularize the grocery list. Instead of creating a list of named data frames, where the name represents the section of the store, let’s create a grocery list where each row represents an item and columns specify the section and price. Because the Tidyverse requires rectangular data, there are several functions which are handy for converting data structures to rectangular form. We could utilize one of these functions to rectangularize the data set. library(tidyverse, quietly = TRUE) groc_rec &lt;- groc %&gt;% bind_rows(., .id = &#39;section&#39;) groc_rec Or, we can simply create the grocery list in rectangular form to begin with. Any feedback for this section? Click here 2.1.1 Reading and Writing Rectangular Data Rectangular data are often stored locally using text files (.txt), comma separated value files (.csv), and Excel files (.xlsx). When data are written to these file types, they are easy to view across devices, without the need for R. Since most grocery store trips obviate the need for R, let’s consider how to write our grocery list to each of these file types. To write and read data to and from text files or comma separated value files, the readr package will come in handy, whereas the xlsx package will allow us to write and read to and from Excel files. To write data from R to a file, we will leverage commands beginning with write. # text file readr::write_delim(groc_rec, path = &#39;./data_raw/groceries-rectangular.txt&#39;) # csv file readr::write_csv(groc_rec, path = &#39;./data_raw/groceries-rectangular.csv&#39;) # Excel file xlsx::write.xlsx(groc_rec, file = &#39;./data_raw/groceries-rectangular.xlsx&#39;, row.names = FALSE) To read data from a file to R, we will leverage commands beginning with read. Before reading data into R, you will need to look at the file and file extension to better understand which function to use. # text file readr::read_delim(&#39;./data_raw/groceries-rectangular.txt&#39;, delim = &#39; &#39;) # csv file readr::read_csv(&#39;./data_raw/groceries-rectangular.csv&#39;) # Excel file xlsx::read.xlsx(&#39;./data_raw/groceries-rectangular.xlsx&#39;, sheetName = &#39;Sheet1&#39;) Reading files into R can sometimes be frustrating. Always look at the data to see if there are column headers and row names. Text files can have different delimiters, characters which separate values in a data set. The default delimiter for readr::write_delim() is a space, but other common text delimiters are tabs, colons, semi-colons, or vertical bars. Commas are so commonly used as a delimiter, it gets a function of its own. Always ensure that data from an Excel spreadsheet are rectangular. Lastly, the readr package will guess the data type of each column. Check these data types are correct using str(). Any feedback for this section? Click here 2.1.2 Reading and Writing Non-rectangular Data Writing non-rectangular data from R to your local machine is easy with the help of write_rds() from the readr package. While the origin of ‘RDS’ is unclear, some believe it stands for R data serialization. Nonetheless, RDS files store single R objects, regardless of the structure. This means that RDS files are a great choice for data which cannot be written to rectangular file formats such as text, csv, and Excel files. The sister function entitled read_rds() allows you to read any RDS file directly into your current R environment, assuming the file already exists. Similar to RDS files, there are also RData files which can store multiple R objects. These files can be written from R to your local machine using save() and read from your local machine to R using load(). We recommend avoiding RData files, and instead, storing multiple R objects in one named list which is then saved as an RDS file. When there is inevitably non-rectangular data that exist which you would like to load into R, you are in for a treat. The rest of this module can loosely be viewed as a guide to managing and curating data. We will leverage many tools to tackle this problem, but in the next two sections, we will address two specif, common instances of non-rectangular data: data from APIs and from scraped sources. Any feedback for this section? Click here "],
["APIs.html", "2.2 APIs: Clean and Curated", " 2.2 APIs: Clean and Curated An application programming interface (API) is a set of functions and procedures which allows one computer program to interact with another. To simplify the concept remarkably, we will consider web-APIs where there is a server (computer waiting to provide data) and a client (computer making a request for data). The benefit of APIs is the result: clean and curated data from the host. The pre-processing needed to get the data in a workable form is entirely done on the server side. We, however, are responsible for making the request. Web-APIs often utilize JavaScript Object Notation (JSON), another example of non-rectangular data. We will utilize the httr and the jsonlite packages to retrieve the latest sports lines from Bovada, an online sportsbook. Before we start, we’ll need to download the httr and jsonlite packages and load them into our current environment. Furthermore, we will need to find the address of the server to which we will send the request. library(httr, quietly = TRUE) library(jsonlite, quietly = TRUE) bov_nfl_api &lt;- &quot;https://www.bovada.lv/services/sports/event/v2/events/A/description/football/nfl&quot; To ask for data through a web-API, we will need to make a GET request with the httr package’s GET() function. After making the request, we can read about the server’s response. bov_req &lt;- httr::GET(url = bov_nfl_api) bov_req Response [https://www.bovada.lv/services/sports/event/v2/events/A/description/football/nfl] Date: 2020-10-14 18:52 Status: 200 Content-Type: application/json;charset=utf-8 Size: 1.16 MB If the request was successful, then the status of the request will read 200. Otherwise, there was some error with your request. For a list of HTTP status codes and their respective definitions, follow this link. Since the response clarifies that the content is indeed driven by JavaScript, then we will utilize the jsonlite package to read the JSON structured data. A handy function we will use will be fromJSON() which converts a character vector containing data in JSON structure to native structures in R like lists. So, in order, we will Extract the content from the server’s response Convert the content to a character vector, maintaining the JSON structure Restructure the data into native R structures, using fromJSON(). content &lt;- bov_req$content content_char &lt;- rawToChar(content) bov_res &lt;- jsonlite::fromJSON(content_char) Of course, we could also create a function which takes the server’s response and converts the content to native R structures. We will want to code in a force stop if the response status is not 200. We will also want to require the httr and jsonlite packages which will automatically install the packages if a user calls the function without having the packages installed. convert_JSON &lt;- function(resp){ # call needed packages require(httr) require(jsonlite) # stop if the server returned an error httr::stop_for_status(resp) # return JSON content in native R structures return(jsonlite::fromJSON(rawToChar(resp$content))) } Finally, we can get the same output by simply calling the function. identical(convert_JSON(bov_req), bov_res) [1] TRUE Some web-APIs require additional information from the us as outlined in the documentation for the API. In this case, the user would need to provide additional query parameters in their GET request. Thankfully, this functionality is ingrained in the httr package’s GET() function. For more information on how to include query parameters, type ??GET into your R console. Any feedback for this section? Click here "],
["scraping-messy-and-mangled.html", "2.3 Scraping: Messy and Mangled", " 2.3 Scraping: Messy and Mangled If you are reading this textbook, at some point in your career, you are likely to want or need data which exists on the web. You have looked for downloadable sources and Google searched for an API, but alas, no luck. The last resort for importing data into R is web scraping. Web scraping is a technique for harvesting data which is portrayed on the web and exists in hypertext markup language (HTML), the language of web browser documents. 2.3.1 Scraping vs APIs The benefit of using an API are clean data. For example, we can traverse the result to find the latest NFL events. head(bov_res[[2]][[1]][,2]) [1] &quot;Atlanta Falcons @ Minnesota Vikings&quot; [2] &quot;Baltimore Ravens @ Philadelphia Eagles&quot; [3] &quot;Chicago Bears @ Carolina Panthers&quot; [4] &quot;Cincinnati Bengals @ Indianapolis Colts&quot; [5] &quot;Cleveland Browns @ Pittsburgh Steelers&quot; [6] &quot;Denver Broncos @ New England Patriots&quot; With more digging, we can find which teams are playing at home. head(bov_res[[2]][[1]][[16]]) [[1]] id name home 1 7783039-11904246 Minnesota Vikings TRUE 2 7783039-11904242 Atlanta Falcons FALSE [[2]] id name home 1 7783032-11904222 Philadelphia Eagles TRUE 2 7783032-11903831 Baltimore Ravens FALSE [[3]] id name home 1 7783035-11904216 Carolina Panthers TRUE 2 7783035-11903832 Chicago Bears FALSE [[4]] id name home 1 7783033-11904232 Indianapolis Colts TRUE 2 7783033-11904217 Cincinnati Bengals FALSE [[5]] id name home 1 7783037-11904223 Pittsburgh Steelers TRUE 2 7783037-11904219 Cleveland Browns FALSE [[6]] id name home 1 7802463-3826 New England Patriots TRUE 2 7802463-2410 Denver Broncos FALSE We can also find the current line of each of these games. Here, I have created a function called get_bovada_lines() which traverses this complicated (yet clean) JSON object using methods explored in Chapter 3 and combines the information together into a rectangular data set. bov_res %&gt;% get_bovada_lines() %&gt;% print(n = 10) # A tibble: 28 x 16 id link description startTime live type lastModified &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dttm&gt; 1 7783~ /foo~ Atlanta Fa~ 2020-10-18 10:00:00 FALSE GAME~ 2020-10-14 10:58:30 2 7783~ /foo~ Atlanta Fa~ 2020-10-18 10:00:00 FALSE GAME~ 2020-10-14 10:58:30 3 7783~ /foo~ Baltimore ~ 2020-10-18 10:00:00 FALSE GAME~ 2020-10-14 09:40:55 4 7783~ /foo~ Baltimore ~ 2020-10-18 10:00:00 FALSE GAME~ 2020-10-14 09:40:55 5 7783~ /foo~ Chicago Be~ 2020-10-18 10:00:00 FALSE GAME~ 2020-10-14 11:38:27 6 7783~ /foo~ Chicago Be~ 2020-10-18 10:00:00 FALSE GAME~ 2020-10-14 11:38:27 7 7783~ /foo~ Cincinnati~ 2020-10-18 10:00:00 FALSE GAME~ 2020-10-14 11:45:21 8 7783~ /foo~ Cincinnati~ 2020-10-18 10:00:00 FALSE GAME~ 2020-10-14 11:45:21 9 7783~ /foo~ Cleveland ~ 2020-10-18 10:00:00 FALSE GAME~ 2020-10-14 10:33:57 10 7783~ /foo~ Cleveland ~ 2020-10-18 10:00:00 FALSE GAME~ 2020-10-14 10:33:57 # ... with 18 more rows, and 9 more variables: team_id &lt;chr&gt;, name &lt;chr&gt;, # home &lt;lgl&gt;, juice_money &lt;dbl&gt;, handicap_spread &lt;dbl&gt;, juice_spread &lt;dbl&gt;, # handicap_total &lt;dbl&gt;, juice_over &lt;dbl&gt;, juice_under &lt;dbl&gt; While traversing these sometimes complicated lists may seem intimidating, with practice, working with data from an API will be made easier after discussing mapping functions in Chapter 3 which are useful for traversing complicated lists. Hopefully, after the scraping section, you will find working with APIs like a walk in the park compared to scraping data directly from the web. 2.3.2 Lessons Learned from Scraping Scraping is a necessary evil that requires patience. While some tasks may prove easy, you will quickly find others seem insurmountable. In this section, we will outline a few tips to help you become a web scraper. Brainstorm! Before jumping into your scraping project, ask yourself what data do I need and where can I find it? If you discover you need data from various sources, what is the unique identifier, the link which ties these data together? Taking the time to explore different websites can save you a vast amount of time in the long run. As a general rule, simplistic looking websites are generally easier to scrape and often contain the same information as more complicated websites with several bells and whistles. Start small! Sometimes a scraping task can feel daunting, but it is important to view your project as a war, splitting it up into small battles. If you are interested in the racial demographics of each of the United States, consider how you can first scrape this information for one state. In this process, don’t forget tip 1! Hyperlinks are your friend! They can lead to websites with more detailed information or serve as the unique identifier you need between different data sources. Sometimes you won’t even need to scrape the hyperlinks to navigate between webpages, making minor adjustments to the web address will sometimes do. Data is everywhere! Text color, font, or highlighting may serve as valuable data that you need. If these features exist on the webpage, then they exist within the HTML code which generated the document. Sometimes these features are well hidden or even inaccessible, leading to the last and final tip. Ready your search engine! Just like coding in R is an art, web developing is an art. When asking distinct developers to create the same website with the same functionality, the final result may be similar but the underlying HTML code could be drastically different. Why does this matter? You will run into an issue that hasn’t been addressed in this text. Thankfully, if you’ve run into an issue, someone else probably has too. We cannot recommend websites like Stack Overflow enough. 2.3.3 Tools for Scraping Before we can scrape information from a webpage, we need a bit of background on how this information is stored and presented. The goal of this subsection is to briefly introduce the languange of the web, hypertext markup language (HTML). When we talk about scraping the web, what we really mean is gathering bits of information from the HTML code used to build a webpage. Like R code, HTML can be overwhelming. The goal is not to teach HTML but to introduce its components, so you have a much more intuitive sense of what we are doing when we scrape the web. 2.3.3.1 Hypertext Markup Language (HTML) Web sites are written in hypertext markup language. All contents that are displayed on a web page are structured through HTML with the help of HTML elements. HTML elements consist of a tag and contents. The tag defines how the web browser should format and display the content. Aptly, the content is what should be displayed. For example, if we wished to format text as a paragraph within the web document, then we could use the paragraph tag, &lt;p&gt;, to indicate the beginning of a paragraph. After opening a tag, we then specify the content to display before closing the tag. A complete paragraph may read: &lt;p&gt; This is the paragraph you want to scrape. &lt;/p&gt; Attributes are optional parameters which provide additional information about the element in which the attribute is included. For example, within the paragraph tag, you can define a class attribute which formats the text in a specific way, such as bolding, coloring, or aligning the text. To extend our example, the element may read: &lt;p class = \"fancy\"&gt; This is the paragraph you want to scrape which has been formatted in a fancy script. &lt;/p&gt; The type of attribute, being class, is the attribute name, whereas the quantity assigned to the attribute, being fancy, is the attribute value. The general decomposition of an HTML element is characterized by the following figure: Figure 2.1: the lingo of an HTML element The class attribute is a flexible one. Many web developers use the class attribute to point to a class name in a style sheet or to access and manipulate elements with the specific class name with a JavaScript. For more information of the class attribute, see this link. For more information on cascading style sheets which are used to decorate HTML pages, see this link. Any feedback for this section? Click here 2.3.3.2 Selector Gadgets While all web pages are composed of HTML elements, the elements themselves can be structured in complicated ways. Elements are often nested inside one another or make use of elements in other documents. These complicated structures can make scraping data difficult. Thankfully, we can circumvent exploring these complicated structures with the help of selector gadgets. A selector gadget allows you to determine what css selector you need to extract the information desired from a webpage. These JavaScript bookmarklets allow you to determine where the information you desire belongs within the complicated structure of elements that makeup a webpage. To follow along in Chapter 3, you will need to download one of these gadgets from this link. If you use Google Chrome, you can download the bookmark extension directly from this link. If the selector gadget fails us, we can always view the structure of the elements directly by viewing the page source. This can be done by right-clicking on the webpage and selecting ‘View Page Source’. For Google Chrome, you can also use the keyboard shortcut ‘CTRL-U’. 2.3.4 Scraping NFL Data In Chapter 2.2, we gathered some betting data pertaining to the NFL through a web-API. We may wish to supplement these betting data with data pertaining to NFL teams, players, or even playing conditions. The goal in this subsection is to introduce you to scraping by heeding the advice given in the Chapter 2.3.2. Further examples are given in the supplemental material. Following our own advice, let’s brainstorm. When you think of NFL data, you probably think of NFL.com or ESPN. These sites obviously have reliable data, but the webpages are pretty involved. While the filters, dropdown menus, and graphics lend great experiences for web browsers, they create headaches for web scrapers. After further digging, we will explore Pro Football Reference, a reliable archive for football statistics (with a reasonably simple webpage). This is an exhaustive source which boasts team statistics, player statistics, and playing conditions for various seasons. Let’s now start small by focusing on team statistics, but further, let’s limit our scope to the 2020 Denver Broncos. Notice, there are hyperlinks for each player documented in any of the categories, as well hyperlinks for each game’s boxscore where there is information about playing conditions and outcomes. Hence, we have a common thread between team statistics, players, and boxscores. If, for example, we chose to scrape team statistics from one website and player statistics from another website, we may have to worry about a unique identifier (being team) if the websites have different naming conventions. 2.3.4.1 HTML Tables: Team Statistics We’ll start with the team statistics for the 2020 Denver Broncos which can be found in a table entitled ‘Team Stats and Rankings’. We’ll need to figure in which element or node the table lives within the underlying HTML. To do this, we will utilize the CSS selector gadget. If we highlight over and click the table with the selector gadget, we will see that the desired table lives in an element called ‘#team_stats’. Figure 2.2: finding the team statistics element using the selector gadget Alternatively, we could view the page source and search for the table name. I’ve highlighted the information identified by the selector gadget with the cursor. Figure 2.3: finding the team statistics element using the page source While the selector gadget is always a great first option, it is not always reliable. There are instances when the selector gadget identifies a node that is hidden or inaccessible without JavaScript. In these situations, it is best view the page source directly for more guidance on how to proceed. Practice with both the selector gadget and the page source. Once we have found the name of the element containing the desired data, we can utilize the rvest package to scrape the table. The general process for scraping an HTML table is Read the HTML identified by the web address. Isolate the node containing the data we desire. Parse the HTML table. Take a look at the data to ensure the columns are appropriate labels. library(rvest) library(janitor) pfr_url &lt;- &quot;https://www.pro-football-reference.com&quot; broncos_url &lt;- str_c(pfr_url, &#39;/teams/den/2020.htm&#39;) broncos_url %&gt;% # read the HTML read_html(.) %&gt;% # isolate the node containing the HTML table html_node(., css = &#39;#team_conversions&#39;) %&gt;% # parse the html table html_table(.) %&gt;% # make the first row of the table column headers and clean up column names row_to_names(., row_number = 1) %&gt;% clean_names() While these data need cleaning up before they can be used in practice, we will defer these responsibilities to Chapter 3. Take this time to scrape the ‘Team Conversions’ table on your own. While it is exciting to scrape your first nuggets of data, we have just scratched the surface of web scraping. To be honest, it took some time to find data this easy to scrape. More often than not, difficulties arise. The HTML table you want may be commented out or hidden. Your data may not be in the form of a table, at all. For a more in-depth exposition of web scraping, see the supplemental materials. Any feedback for this section? Click here "],
["Wrangling.html", "Chapter 3 Wrangling", " Chapter 3 Wrangling “The work that you do with data wrangling others would call ‘data plumbing’ or even janitorial work, but when you have somebody who knows how to wrangle data and gets into a flow of data wrangling, it’s an elegant dance to watch.” —Stephanie McReynolds, Strategic Advisor, Nexla In Chapter 2, we introduced the idea of rectangular data vs. non-rectangular data, providing examples for each and demonstrating the process of rectangularization. We outlined how to use a web-API before a light introduction to web scraping. In this chapter, we will famaliarize ourselves with data wrangling, the art of cleaning up our data. Furthermore, in Chapter 2, we briefly touched on the notion of tidy data: data structures where observations are given by rows, variables are given by columns, and values are given by cells. The notion of tidy data was formalized by Wickham and others (2014), Chief Scientist at RStudio and creator of the Tidyverse universe, to reduce the amount of work involved in preparing data. Data preparation, or data cleaning, is often a time consuming task with real data. Since it’s necessary to format data as tidy data to use the vast network of packages within the Tidyverse, we always need to first structure our data as tidy data. Figure 3.1: visualizing tidy data In the following subsections, we will be cleaning up those data gathered from Pro Football Reference. Any feedback for this section? Click here References "],
["core-tidyverse.html", "3.1 Core Tidyverse", " 3.1 Core Tidyverse At the heart of Tidyverse are a few packages: dplyr, tidyr, stringr, and forcats. Each package serves a powerful purpose. tidyr helps you create tidy data, while dplyr is used for data manipulation. Think of tidyr as the crowbar or saw in your toolbox, allowing you to bend and shape your data into tidy shape. dplyr, on the other hand, is more like the screwdriver, hammer, or level, allowing you to fix pesky issues with the data. stringr and forcats are useful when working with strings and factors (categorical variables that have a fixed and known set of possible values). While we will make the effort to teach tidyr and dplyr seperately within their own subsections, we recognize that these packages are ultimately created to be used together. Since we may need to make use of functions across packages, we will explicitly state the origin each function by package::function(). 3.1.1 tidyr Let’s take a look at the game by game player rushing and receiving statistics that we scraped using the principles outlined in the previous chapter. To do this, we will first use stringr::str_c(), a function we’ve seen a few times now, to create a web addresses corresponding to Pro Football Reference page for all of the 2020 NFL teams. pfr_url &lt;- &quot;https://www.pro-football-reference.com&quot; team_urls &lt;- pfr_url %&gt;% # get team abbreviations for all NFL teams and isolate Denver get_teams(.) %&gt;% # create URLs for all 2020 NFL teams stringr::str_c(pfr_url, ., &#39;2020.htm&#39;) as_tibble(team_urls) Pipes (%&gt;%) make your code much more readable and avoid unnessary assignments. While not required in many cases, I use a period to indicate where the result to the left of the pipe belongs in the argument to the right of the pipe. Now that we have the web addresses for every 2020 NFL team, let’s isolate the 2020 Denver Broncos and scrape their players’ aggregated rushing and receiving statistics. We will use dplyr::glimpse() to see the data types and a few values for each column in the data set. den_stats &lt;- team_urls %&gt;% # isolate the 2020 Denver Bronco&#39;s URL .[10] %&gt;% # get team statistics for 2020 Denver Broncos get_team_stats() dplyr::glimpse(den_stats) Rows: 19 Columns: 27 $ no &lt;chr&gt; &quot;25&quot;, &quot;28&quot;, &quot;30&quot;, &quot;9&quot;, &quot;4&quot;, &quot;3&quot;, &quot;13&quot;, &quot;6&quot;, ... $ player &lt;chr&gt; &quot;Melvin Gordon&quot;, &quot;Royce Freeman&quot;, &quot;Phillip L... $ age &lt;chr&gt; &quot;27&quot;, &quot;24&quot;, &quot;26&quot;, &quot;27&quot;, &quot;24&quot;, &quot;24&quot;, &quot;21&quot;, &quot;3... $ pos &lt;chr&gt; &quot;rb&quot;, &quot;&quot;, &quot;rb&quot;, &quot;qb&quot;, &quot;qb&quot;, &quot;qb&quot;, &quot;wr&quot;, &quot;p&quot;,... $ games_g &lt;chr&gt; &quot;4&quot;, &quot;4&quot;, &quot;1&quot;, &quot;3&quot;, &quot;2&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;4&quot;,... $ games_gs &lt;chr&gt; &quot;4&quot;, &quot;0&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;2&quot;, &quot;2&quot;, &quot;0&quot;, &quot;4&quot;,... $ rushing_att &lt;chr&gt; &quot;65&quot;, &quot;9&quot;, &quot;7&quot;, &quot;6&quot;, &quot;5&quot;, &quot;3&quot;, &quot;2&quot;, &quot;1&quot;, &quot;0&quot;... $ rushing_yds &lt;chr&gt; &quot;281&quot;, &quot;30&quot;, &quot;24&quot;, &quot;28&quot;, &quot;-5&quot;, &quot;5&quot;, &quot;7&quot;, &quot;0&quot;... $ rushing_td &lt;chr&gt; &quot;3&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;,... $ rushing_lng &lt;chr&gt; &quot;43&quot;, &quot;13&quot;, &quot;10&quot;, &quot;9&quot;, &quot;-1&quot;, &quot;3&quot;, &quot;9&quot;, &quot;0&quot;, ... $ rushing_y_a &lt;chr&gt; &quot;4.3&quot;, &quot;3.3&quot;, &quot;3.4&quot;, &quot;4.7&quot;, &quot;-1.0&quot;, &quot;1.7&quot;, &quot;... $ rushing_y_g &lt;chr&gt; &quot;70.3&quot;, &quot;7.5&quot;, &quot;24.0&quot;, &quot;9.3&quot;, &quot;-2.5&quot;, &quot;2.5&quot;,... $ rushing_a_g &lt;chr&gt; &quot;16.3&quot;, &quot;2.3&quot;, &quot;7.0&quot;, &quot;2.0&quot;, &quot;2.5&quot;, &quot;1.5&quot;, &quot;... $ rushing_fmb &lt;chr&gt; &quot;1&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;1&quot;, &quot;3&quot;, &quot;0&quot;, &quot;1&quot;, &quot;0&quot;,... $ receiving_tgt &lt;chr&gt; &quot;15&quot;, &quot;5&quot;, &quot;1&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;12&quot;, &quot;&quot;, &quot;27&quot;, ... $ receiving_rec &lt;chr&gt; &quot;11&quot;, &quot;5&quot;, &quot;1&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;6&quot;, &quot;&quot;, &quot;19&quot;, &quot;... $ receiving_yds &lt;chr&gt; &quot;45&quot;, &quot;49&quot;, &quot;11&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;78&quot;, &quot;&quot;, &quot;219... $ receiving_y_r &lt;chr&gt; &quot;4.1&quot;, &quot;9.8&quot;, &quot;11.0&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;13.0&quot;, &quot;&quot;... $ receiving_td &lt;chr&gt; &quot;1&quot;, &quot;0&quot;, &quot;0&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;0&quot;, &quot;&quot;, &quot;2&quot;, &quot;2&quot;... $ receiving_lng &lt;chr&gt; &quot;16&quot;, &quot;28&quot;, &quot;11&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;18&quot;, &quot;&quot;, &quot;31&quot;... $ receiving_r_g &lt;chr&gt; &quot;2.8&quot;, &quot;1.3&quot;, &quot;1.0&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;2.0&quot;, &quot;&quot;, ... $ receiving_y_g &lt;chr&gt; &quot;11.3&quot;, &quot;12.3&quot;, &quot;11.0&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;26.0&quot;, ... $ receiving_ctch_percent &lt;chr&gt; &quot;73.3%&quot;, &quot;100.0%&quot;, &quot;100.0%&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;50... $ receiving_y_tgt &lt;chr&gt; &quot;3.0&quot;, &quot;9.8&quot;, &quot;11.0&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;6.5&quot;, &quot;&quot;,... $ yds_touch &lt;chr&gt; &quot;76&quot;, &quot;14&quot;, &quot;8&quot;, &quot;6&quot;, &quot;5&quot;, &quot;3&quot;, &quot;8&quot;, &quot;1&quot;, &quot;1... $ yds_y_tch &lt;chr&gt; &quot;4.3&quot;, &quot;5.6&quot;, &quot;4.4&quot;, &quot;4.7&quot;, &quot;-1.0&quot;, &quot;1.7&quot;, &quot;... $ yds_y_scm &lt;chr&gt; &quot;326&quot;, &quot;79&quot;, &quot;35&quot;, &quot;28&quot;, &quot;-5&quot;, &quot;5&quot;, &quot;85&quot;, &quot;0... There are a few things to note after looking at the data, but let’s first consider if these data are tidy. To answer this question, we need more context. If we wished to predict a player’s position based on his statistics, then each player is an observation and his statistics are variables. In this case, these data are tidy in their current wide form, when each player has information in only one row. If we are interested in comparing players across the recorded statistical categories, then an observation would no longer be a player but a player’s statistical category. In this case, these data are tidy in long form, when each player has information in multiple rows. No matter the context, tidyr makes it easy convert your data from wide form to long form with tidyr::pivot_longer() and from long form to wide form with tidyr::pivot_wider(). Figure 3.2: visualizing the transformation from wide data to long data, and vice versa Let’s consider how we can use tidyr::pivot_longer() to tidy these data in preperation for comparing players across their statistical categories. In each row, we should expect to have (1) the player’s name, age, and position, (2) the statistical category, and (3) the value of the statistical category. Imagine taking every other column and pushing them into the rows, duplicating the information above as needed. To do this in R, we will use the dplyr functions dplyr::vars() which allows us to select which variables to push into the rows and dplyr::starts_with() which allows us to pick only variables whose names start with some string. These are two handy functions to know. Further, we will use tidyr::seperate to split the statistical categories into a more general category (i.e. rushing, receiving, etc) and a statistic (i.e. yards per rushing attempt, catch percentage, etc). den_stats_long &lt;- den_stats %&gt;% # push columns into the rows, renaming the names and values columns pivot_longer(., cols = c(starts_with(&#39;games_&#39;), starts_with(&#39;rushing_&#39;), starts_with(&#39;receiving_&#39;), starts_with(&#39;yds_&#39;)), names_to = &#39;stat_category&#39;, values_to = &#39;value&#39;) %&gt;% # seperate the stat category into a category column and a stat column separate(., col = &#39;stat_category&#39;, into = c(&#39;category&#39;, &#39;stat&#39;), sep = &#39;_&#39;, extra = &#39;merge&#39;) den_stats_long If we are in a situation when we are given a data set in long form, but we need it in wide form, we can use tidyr::pivot_wider. den_stats_wide &lt;- den_stats_long %&gt;% pivot_wider(., names_from = c(&#39;category&#39;, &#39;stat&#39;), values_from = &#39;value&#39;) den_stats_wide After converting back to wide form, the result is the same as the original data set. 3.1.2 dplyr We’ve addressed how to change the shape of your data using tidyr. Now, we will transition into dplyr where we will outline some of the many functions that can prove helpful in preparing your data for plotting or modeling. Before we jump into various functions, let’s outline the issues with our data set which have not been addressed. The best way to get a sense of the issues is to look at the data. den_stats str(den_stats) &#39;data.frame&#39;: 19 obs. of 27 variables: $ no : chr &quot;25&quot; &quot;28&quot; &quot;30&quot; &quot;9&quot; ... $ player : chr &quot;Melvin Gordon&quot; &quot;Royce Freeman&quot; &quot;Phillip Lindsay&quot; &quot;Jeff Driskel&quot; ... $ age : chr &quot;27&quot; &quot;24&quot; &quot;26&quot; &quot;27&quot; ... $ pos : chr &quot;rb&quot; &quot;&quot; &quot;rb&quot; &quot;qb&quot; ... $ games_g : chr &quot;4&quot; &quot;4&quot; &quot;1&quot; &quot;3&quot; ... $ games_gs : chr &quot;4&quot; &quot;0&quot; &quot;1&quot; &quot;1&quot; ... $ rushing_att : chr &quot;65&quot; &quot;9&quot; &quot;7&quot; &quot;6&quot; ... $ rushing_yds : chr &quot;281&quot; &quot;30&quot; &quot;24&quot; &quot;28&quot; ... $ rushing_td : chr &quot;3&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ... $ rushing_lng : chr &quot;43&quot; &quot;13&quot; &quot;10&quot; &quot;9&quot; ... $ rushing_y_a : chr &quot;4.3&quot; &quot;3.3&quot; &quot;3.4&quot; &quot;4.7&quot; ... $ rushing_y_g : chr &quot;70.3&quot; &quot;7.5&quot; &quot;24.0&quot; &quot;9.3&quot; ... $ rushing_a_g : chr &quot;16.3&quot; &quot;2.3&quot; &quot;7.0&quot; &quot;2.0&quot; ... $ rushing_fmb : chr &quot;1&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ... $ receiving_tgt : chr &quot;15&quot; &quot;5&quot; &quot;1&quot; &quot;&quot; ... $ receiving_rec : chr &quot;11&quot; &quot;5&quot; &quot;1&quot; &quot;&quot; ... $ receiving_yds : chr &quot;45&quot; &quot;49&quot; &quot;11&quot; &quot;&quot; ... $ receiving_y_r : chr &quot;4.1&quot; &quot;9.8&quot; &quot;11.0&quot; &quot;&quot; ... $ receiving_td : chr &quot;1&quot; &quot;0&quot; &quot;0&quot; &quot;&quot; ... $ receiving_lng : chr &quot;16&quot; &quot;28&quot; &quot;11&quot; &quot;&quot; ... $ receiving_r_g : chr &quot;2.8&quot; &quot;1.3&quot; &quot;1.0&quot; &quot;&quot; ... $ receiving_y_g : chr &quot;11.3&quot; &quot;12.3&quot; &quot;11.0&quot; &quot;&quot; ... $ receiving_ctch_percent: chr &quot;73.3%&quot; &quot;100.0%&quot; &quot;100.0%&quot; &quot;&quot; ... $ receiving_y_tgt : chr &quot;3.0&quot; &quot;9.8&quot; &quot;11.0&quot; &quot;&quot; ... $ yds_touch : chr &quot;76&quot; &quot;14&quot; &quot;8&quot; &quot;6&quot; ... $ yds_y_tch : chr &quot;4.3&quot; &quot;5.6&quot; &quot;4.4&quot; &quot;4.7&quot; ... $ yds_y_scm : chr &quot;326&quot; &quot;79&quot; &quot;35&quot; &quot;28&quot; ... The first issue with the data set that we should address is the inappropriate data types assigned to each column. Every column is scraped as a character, but the players’ position (pos) should be coded as a factor variable and the players’ statistics should be coded as numeric variables. Before we can change the players’ catch percentage (receiving_ctch_percent) to a numeric variable, we first need to remove the percent sign from the values of the variable. 3.1.2.1 mutate When we wish to change the values within a column, we can leverage the dplyr::mutate() function. Let’s see how we can use some stringr functions within dplyr::mutate() to clean up some of these columns. den_stats_wking &lt;- den_stats %&gt;% mutate(., age = as.numeric(age), pos = pos %&gt;% str_to_upper() %&gt;% na_if(., &#39;&#39;) %&gt;% as_factor(), receiving_ctch_percent = str_remove_all(receiving_ctch_percent, &#39;%&#39;)) den_stats_wking Within one mutate() call, we (1) recoded the players’ age as a numeric variable, (2) changed the players’ position to uppercase, replaced empty strings with NA values, and recoded the result as a factor, and (3) removed all percent signs from the catch percent statistic. We still need to change all of the players’ statistics to numeric variable. We could do this similarly to the players’ age, but listing out every variable one by one would be painstakingly inefficent. To complete this task, we can use a commmon variant of the mutate() function called mutate_at() to change every specified column in a similar manner. We can then use mutate_all() to replace every missing string with an NA value, the proper way to specify a missing value in R. den_stats_wking &lt;- den_stats_wking %&gt;% mutate_at(., vars(starts_with(&#39;games_&#39;), starts_with(&#39;rushing_&#39;), starts_with(&#39;receiving_&#39;), starts_with(&#39;yds_&#39;)), ~as.numeric(.x)) %&gt;% mutate_all(., ~na_if(.x, &#39;&#39;)) These variants of mutate() (e.g. mutate_at and mutate_all) are aimed to condense code when the same transformation is applied to many columns. To specify the transformation which will be applied to all of the identified columns, you will need a lambda expression. The lambda expressions start with ~ and are followed by a function specifying the transformation such as as.numeric. One of the arguments of the function will be .x which indicates an arbitary column. To break down the mutate_at() specified above, we seek to transform all columns starting with games_, rushing_, receiving_, and yds_. Overall, there are 23 columns we will be transforming with this single mutate function. The lambda expression ~as.numeric(.x) specifies how to transform each of these 23 columns. That is, for a given column called .x, change .x to be a numeric column. For more information on these variants, also known as scoped verbs (e.g. _if, _at, _all), type ?mutate_if into the console. Several functions we will discuss throughout this chapter also can be used with one of these scoped verbs. 3.1.2.2 slice and filter Notice, at the bottom of the data set there are two rows which summarize the table. We do not need these rows since they do not outline the performance of an individual player. Keeping these rows would violate the principle of tidy data. There are two approaches we can take to remedy this issue. First, we could consider removing the last two rows of the data set or keeping all rows except for the last two. When we would like to remove or keep rows of a data set using the index of the row, we can leverage the slice() function. To illustrate how to do this, we will use a new helper function n() which returns the number of rows in the referenced data set. # option 1: remove the last two rows of the data set den_stats_wking %&gt;% slice(., -(n()-1), -n()) # or den_stats_wking %&gt;% slice(., -c(n()-1, n())) # option 2: keep all rows except for the last two den_stats_wking %&gt;% slice(., 1:(n()-2)) While this certainly solves the issue for this particular data set, it is not a robust solution. If we hope to apply this same logic for other NFL teams, we need to recognize that this solution relies on there only being two total columns which take up the last two rows of the data set. A more robust solution would be to keep only rows consisting of a player not named \"Team Total\" or \"Opp Total\". If we want to choose rows based on specific criteria, then we can utilize filter() to choose rows based on specific criteria. den_stats_wking &lt;- den_stats_wking %&gt;% filter(., !(player %in% c(&#39;Team Total&#39;, &#39;Opp Total&#39;))) den_stats_wking 3.1.2.3 select "],
["functional-programming-with-purrr.html", "3.2 Functional Programming with purrr", " 3.2 Functional Programming with purrr "],
["iterative-procedures-with-foreach.html", "3.3 Iterative Procedures with foreach", " 3.3 Iterative Procedures with foreach "],
["supplemental.html", "Chapter 4 Supplemental", " Chapter 4 Supplemental “You can have data without information, but you cannot have information without data.” — Daniel Keys Moran, Computer Scientist and Author "],
["scraping-in-the-wild.html", "4.1 Scraping in the Wild", " 4.1 Scraping in the Wild In Chapter 2, we introduced the idea of rectangular data vs. non-rectangular data, providing examples for each and demonstrating the process of rectangularization. We outlined how to use a web-API before introducing the concept of web scraping by illustrating the language of the web: HTML. Since webpages can be complicated, scraping can be complicated. In this chapter, we will leverage the Selector Gadget and our knowledge of HTML elements to scrape data from various sources. It is our belief that the only way to teach web scraping is through examples. Each example will become slightly more difficult than the previous. 4.1.1 Lessons Learned from Scraping Scraping is a necessary evil that requires patience. While some tasks may prove easy, you will quickly find others seem insurmountable. In this section, we will outline a few tips to help you become a web scraper. Brainstorm! Before jumping into your scraping project, ask yourself what data do I need and where can I find it? If you discover you need data from various sources, what is the unique identifier, the link which ties these data together? Taking the time to explore different websites can save you a vast amount of time in the long run. As a general rule, simplistic looking websites are generally easier to scrape and often contain the same information as more complicated websites with several bells and whistles. Start small! Sometimes a scraping task can feel daunting, but it is important to view your project as a war, splitting it up into small battles. If you are interested in the racial demographics of each of the United States, consider how you can first scrape this information for one state. In this process, don’t forget tip 1! Hyperlinks are your friend! They can lead to websites with more detailed information or serve as the unique identifier you need between different data sources. Sometimes you won’t even need to scrape the hyperlinks to navigate between webpages, making minor adjustments to the web address will sometimes do. Data is everywhere! Text color, font, or highlighting may serve as valuable data that you need. If these features exist on the webpage, then they exist within the HTML code which generated the document. Sometimes these features are well hidden or even inaccessible, leading to the last and final tip. Ready your search engine! Just like coding in R is an art, web developing is an art. When asking distinct developers to create the same website with the same functionality, the final result may be similar but the underlying HTML code could be drastically different. Why does this matter? You will run into an issue that hasn’t been addressed in this text. Thankfully, if you’ve run into an issue, someone else probably has too. We cannot recommend websites like Stack Overflow enough. 4.1.2 Scraping NFL Data In Chapter 2, we gathered some betting data pertaining to the NFL through a web-API. We may wish to supplement these betting data with data pertaining to NFL teams, players, or even playing conditions. As we progress through this sub-section, examples will become increasingly problematic or troublesome. The goal in this subsection is to introduce you to scraping by heeding the advice given in the Chapter 4.1.1. Following our own advice, let’s brainstorm. When you think of NFL data, you probably think of NFL.com or ESPN. After further digging, we will explore Pro Football Reference, a reliable archive for football statistics (with a reasonably simple webpage). This is an exhaustive source which boasts team statistics, player statistics, and playing conditions for various seasons. Let’s now start small by focusing on team statistics, but further, let’s limit our scope to the 2020 Denver Broncos. Notice, there are hyperlinks for each player documented in any of the categories, as well hyperlinks for each game’s boxscore where there is information about playing conditions and outcomes. Hence, we have a common thread between team statistics, players, and boxscores. If, for example, we chose to scrape team statistics from one website and player statistics from another website, we may have to worry about a unique identifier (being team) if the websites have different naming conventions. 4.1.2.1 HTML Tables: Team Statistics We’ll start with the team statistics for the 2020 Denver Broncos which can be found in a table entitled ‘Team Stats and Rankings’. We’ll need to figure in which element or node the table lives within the underlying HTML. To do this, we will utilize the CSS selector gadget. If we highlight over and click the table with the selector gadget, we will see that the desired table lives in an element called ‘#team_stats’. Figure 4.1: finding the team statistics element using the selector gadget Alternatively, we could view the page source and search for the table name. I’ve highlighted the information identified by the selector gadget with the cursor. Figure 4.2: finding the team statistics element using the page source While the selector gadget is always a great first option, it is not always reliable. There are instances when the selector gadget identifies a node that is hidden or inaccessible without JavaScript. In these situations, it is best view the page source directly for more guidance on how to proceed. Practice with both the selector gadget and the page source. Once we have found the name of the element containing the desired data, we can utilize the rvest package to scrape the table. The general process for scraping an HTML table is Read the HTML identified by the web address. Isolate the node containing the data we desire. Parse the HTML table. Take a look at the data to ensure the columns are appropriate labels. library(rvest) library(janitor) pfr_url &lt;- &quot;https://www.pro-football-reference.com&quot; broncos_url &lt;- str_c(pfr_url, &#39;/teams/den/2020.htm&#39;) broncos_url %&gt;% # read the HTML read_html(.) %&gt;% # isolate the node containing the HTML table html_node(., css = &#39;#team_conversions&#39;) %&gt;% # parse the html table html_table(.) %&gt;% # make the first row of the table column headers and clean up column names row_to_names(., row_number = 1) %&gt;% clean_names() While these data need cleaning up before they can be used in practice, we will defer these responsibilities to Chapter 3. In the next subsection, we will take a deeper dive into scraping HTML tables using information attained from attribute values, a common occurrence in web scraping. Take this time to scrape the ‘Team Conversions’ table on your own. Any feedback for this section? Click here 4.1.2.2 Commented and Hidden HTML Tables: Player Statistics Let’s transition to gathering player statistics, particularly for those players who have recorded statistics in the rushing or receiving category. These data are given in the table entitled ‘Rushing and Receiving’. Let’s use the selector gadget to identify the node containing the table and scrape the table according to the previous section. broncos_url %&gt;% # read the HTML read_html(.) %&gt;% # isolate the node containing the HTML table html_node(., css = &#39;#rushing_and_receiving&#39;) %&gt;% # parse the html table html_table(.) Error in UseMethod(\"html_table\"): no applicable method for 'html_table' applied to an object of class \"xml_missing\" We get an error stating that the node ‘#rushing_and_receiving’ does not contain an HTML table. In fact, there doesn’t appear to be anything in that node at all. broncos_url %&gt;% # read the HTML read_html(.) %&gt;% # isolate the node containing the HTML table html_node(., css = &#39;#rushing_and_receiving&#39;) {xml_missing} &lt;NA&gt; This, of course, is contrary to the selector gadget declaring a node ‘#rushing_and_receiving’ as the one containing the table we desire and directly see on the webpage. Generally when this happens, it means the node containing the information in the table has either been commented out or hidden. A node is commented out when it is contained in a comment tag: &lt;!-- --&gt;. For example, if we were to comment out the paragraph element in Chapter 2, it would look like this: &lt;!-- &lt;p class = \"fancy\"&gt; This is the paragraph you want to scrape which has been formatted in a fancy script. &lt;/p&gt; --&gt;. When a node is commented out, it exists in the HTML, but it cannot be accessed until we step into the comment tag. Otherwise said, we can’t bypass the comment node. If you would like to scrape information contained with a comment tag, the general strategy is Read the HTML identified by the web address. Isolate the node containing the comment tag. Inside the comment tag are the data we desire. Isolate the comment tag. Convert the content to HTML. Isolate the node containing the table. Parse the HTML table. Take a look at the data to ensure the columns are appropriate labels. broncos_url %&gt;% read_html() %&gt;% html_node(., css = &#39;#all_rushing_and_receiving&#39;) %&gt;% html_nodes(., xpath = &#39;comment()&#39;) %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(., &#39;table&#39;) %&gt;% html_table() %&gt;% set_names(., str_c(names(.), .[1,], sep = &#39;_&#39;)) %&gt;% clean_names() %&gt;% slice(., -1) 4.1.2.3 Attributes: Player Statistics by Game Suppose now that we wish to scrape the same player statistics, but rather than get the aggregated total, we want the players’ statistics by game. Within the Rushing and Receiving table scraped in the previous example, there are hyperlinks for each player. When you click on this hyperlink, you will find the player’s statistics for each game played. These are the data we desire. We want rushing and receiving statistics for each player on each team for a variety of years. Using the lessons learned in Chapter 4.1.1, we have brainstormed the problem and have a road map to the solution. We will first scrape the hyperlink attribute to get the web address associated with each player for a given team. Within each of these web addresses, we will scrape the game by game statistics before iterating over each team in the NFL. After this, we can repeat the procedure for a variety of years. This is, of course, a big job, so let’s scale down the problem to one player: Melvin Gordon III, running back. To do this, we will: Get the web addresses associated with each player in the Rushing and Receiving table, before isolating the address associated with Melvin Gordon’s rushing and receiving statistics. Read the HTML identified by the web address. Isolate the node containing the data we desire. Parse the HTML table. Take a look at the data to ensure the columns are appropriate labels. Let’s figure out where the hyperlinks exist within the HTML code. To do this, I will look at the page source and search for Melvin Gordon. We find that the web address corresponding to his game by game player statistics exist in the hidden HTML table that we scraped in the previous example. In particular, they are the href attribute to a node entitled a which is embedded within the hidden HTML table. Phew. Let’s consider how we can scrape these web addresses step by step: 1.1 Isolate the hidden HTML table, similar to the previous example. 1.2 Isolate nodes with tag a. 1.3 Extract the href attribute from these nodes. Steps 1.1 and 1.2 should be familiar from previous examples, but step 1.3 requires a new function within rvest called html_attr. Let’s see how to do this in R. Figure 4.3: finding the element containing the web address cooresponding to Melvin Gordon III game by game player statistics using the page source player_urls &lt;- broncos_url %&gt;% read_html() %&gt;% html_node(., css = &#39;#all_rushing_and_receiving&#39;) %&gt;% html_nodes(., xpath = &#39;comment()&#39;) %&gt;% html_text() %&gt;% read_html() %&gt;% html_nodes(., &#39;table&#39;) %&gt;% html_nodes(., &#39;a&#39;) %&gt;% html_attr(., &#39;href&#39;) %&gt;% str_c(pfr_url, .) player_urls [1] &quot;https://www.pro-football-reference.com/players/G/GordMe00.htm&quot; [2] &quot;https://www.pro-football-reference.com/players/F/FreeRo00.htm&quot; [3] &quot;https://www.pro-football-reference.com/players/L/LindPh00.htm&quot; [4] &quot;https://www.pro-football-reference.com/players/D/DrisJe00.htm&quot; [5] &quot;https://www.pro-football-reference.com/players/R/RypiBr00.htm&quot; [6] &quot;https://www.pro-football-reference.com/players/L/LockDr00.htm&quot; [7] &quot;https://www.pro-football-reference.com/players/H/HamlKJ00.htm&quot; [8] &quot;https://www.pro-football-reference.com/players/M/MartSa01.htm&quot; [9] &quot;https://www.pro-football-reference.com/players/F/FantNo00.htm&quot; [10] &quot;https://www.pro-football-reference.com/players/P/PatrTi00.htm&quot; [11] &quot;https://www.pro-football-reference.com/players/J/JeudJe00.htm&quot; [12] &quot;https://www.pro-football-reference.com/players/H/HamiDa01.htm&quot; [13] &quot;https://www.pro-football-reference.com/players/S/SuttCo00.htm&quot; [14] &quot;https://www.pro-football-reference.com/players/B/ButtJa00.htm&quot; [15] &quot;https://www.pro-football-reference.com/players/V/VannNi00.htm&quot; [16] &quot;https://www.pro-football-reference.com/players/C/ClevTy00.htm&quot; [17] &quot;https://www.pro-football-reference.com/players/S/SpenDi00.htm&quot; Since the web addresses are given as an extension to the Pro Football Reference homepage, we will need to concatenate the strings with str_c() before saving the result for future use. Now, lets continue to step two in our excursion to get Melvin Gordon’s game by game statistics. player_urls[1] %&gt;% read_html() %&gt;% html_node(., css = &#39;#all_stats&#39;) %&gt;% html_node(., &#39;table&#39;) %&gt;% html_table(., fill = TRUE) %&gt;% set_names(., str_c(names(.), .[1,], sep = &#39;_&#39;)) %&gt;% clean_names() %&gt;% slice(., -1) Any feedback for this section? Click here 4.1.3 Putting It All Together Let’s consider how we can gather data regarding racial demographics across each state in the United States of America. Perusing Wikipedia, notice that each state has a table documenting the state’s ‘racial population breakdown’. Figure 4.4: part of brainstorming is exploring different websites which may have the data you desire If we were to go to Georgia, we would notice that the web address is similar to Alabama, but since Georgia is also a country, the web address specifies that you are currently on the webpage for the state rather than the country. This effectively rules out editing the web address to iterate over each state. If we broaden our search to ‘US States’, we find a Wikipedia page with a list of each state. Each state has a hyperlink which takes us to the state’s webpage with the demographic information we desire. Remember: hyperlinks are our friend! If they exist, we can scrape them, iterating over the web addresses to get data from each state. Let’s use what we just learned to attain the web addresses for each state. We’ll start with the web address containing a list of every state. wiki_url &lt;- &quot;https://www.wikipedia.org&quot; states_url &lt;- str_c(wiki_url, &#39;/wiki/U.S._state#States_of_the_United_States&#39;) Let’s use the page source to find where the hyperlinks exist in the HTML code. To do this, I am going to search for the phrase ‘The 50 U.S. states, in alphabetical order, along with each state’s flag’ in the page source since the list of states is just below this phrase. Figure 4.5: finding the element containing the list of states with their cooresponding web addresses using the page source We find that the list of states, among other information, exists in an element tagged div with the attribute class set to an attribute value of plainlist. Within this node, the list of states exists in an element tagged ul and each state exists in an element tagged li. Within each state, the web address corresponding to that state’s individual Wikipedia article is given in the href attribute to an element tagged a. Let’s use this information to extract the web addresses corresponding to each state. We may also want to extract the state name which is the content of the element tagged a. node_w_states &lt;- states_url %&gt;% read_html() %&gt;% html_nodes(xpath = &quot;//div[@class=&#39;plainlist&#39;]&quot;) %&gt;% html_nodes(&#39;ul&#39;) %&gt;% html_nodes(&#39;li&#39;) %&gt;% html_nodes(&#39;a&#39;) ind_states &lt;- tibble(state = node_w_states %&gt;% html_text(), url = node_w_states %&gt;% html_attr(., &#39;href&#39;) %&gt;% str_c(wiki_url, .)) ind_states Let’s consider how we would scrape the racial demographics. Use either the selector gadget or the page source to identify where the desired table exists in the HTML code. ind_states$url[1] %&gt;% read_html() %&gt;% html_nodes(., xpath = &quot;//table[@class=&#39;wikitable sortable collapsible&#39;]&quot;) %&gt;% html_table() %&gt;% as.data.frame() In the next chapter, we will learn the tools to scrape the racial demographics for each state using the vector of web addresses we attained through this process as well as cleaning up these data according to tidy principles. Any feedback for this section? Click here "],
["references.html", "References", " References "]
]
