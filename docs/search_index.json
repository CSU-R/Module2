[
["index.html", "R Module 2 Chapter 1 Welcome!", " R Module 2 Connor Gibbs1 30 Sep, 2020, 12:51 PM Chapter 1 Welcome! Hi, and welcome to the R Module 2 (AKA STAT 158) course at Colorado State University! This course is the second of three 1 credit courses intended to introduce the R programming language, specifically the Tidyverse. Through these Modules (courses), we’ll explore how R can be used to do the following: Access data via files or web application programming interfaces (APIs) Scrape data from web Wrangle and clean complicated data structures Create graphics with an eye for quality and aesthetics Understand data using basic modeling In addition, you’ll also be exposed to broader concepts, including: Data organization and storage Hypertext Markup Language (HTML) Tidyverse principles More detail will be provided in the Course Topics laid out in the next chapter. 1.0.1 How To Navigate This Book To move quickly to different portions of the book, click on the appropriate chapter or section in the the table of contents on the left. The buttons at the top of the page allow you to show/hide the table of contents, search the book, change font settings, download a pdf or ebook copy of this book, or get hints on various sections of the book. The faint left and right arrows at the sides of each page (or bottom of the page if it’s narrow enough) allow you to step to the next/previous section. Here’s what they look like: Figure 1.1: Left and right navigation arrows Department of Statistics, Colorado State University, connor.gibbs@colostate.edu↩ "],
["associated-csu-course.html", "1.1 Associated CSU Course", " 1.1 Associated CSU Course This bookdown book is intended to accompany the associated course at Colorado State University, but the curriculum is free for anyone to access and use. If you’re reading the PDF or EPUB version of this book, you can find the “live” version at https://csu-r.github.io/Module2/, and all of the source files for this book can be found at https://github.com/CSU-R/Module2. If you’re not taking the CSU course, you will periodically encounter instructions and references which are not relevant to you. For example, we will make reference to the Canvas website, which only CSU students enrolled in the course have access to. "],
["AccessingData.html", "Chapter 2 Accessing Data", " Chapter 2 Accessing Data “Data is the new oil.” —Clive Humby, Chief Data Scientist, Starcount In this chapter, we’ll cover how to access data given in various forms and provided from various sources. "],
["rectangular-vs-non-rectangular-data.html", "2.1 Rectangular vs. Non-rectangular Data", " 2.1 Rectangular vs. Non-rectangular Data Data present themselves in many forms, but at a basic level, all data can be categorized into two structures: rectangular data and non-rectangular data. Intuitively, rectangular data are shaped like a rectangle where every value corresponds to some row and column. Non-rectangular data, on the other hand, are not no neatly arranged in rows and columns. Instead, they are often a culmination of separate data structures where there is some similarity among members of the same data structure. To motivate this idea, let’s consider a basic grocery list which consists of ten items: black beans, milk, pasta, cheese, bananas, peanut butter, bread, apples, tomato sauce, and mayonnaise. Notice, there is little organization to this list, and more involved shoppers may find this list inadequate or unhelpful. We may wish to group these items by sections in which we’re likely to find them. We may also want to include prices, so we know in-store whether the items are on sale. Let’s consider two distinct (but legitimate) ways to organize these data. To illustrate the idea of rectangular vs. non-rectangular data, we will consider how these data can be structured in both ways using R. You may not have seen some of these functions yet. No worries! The objective is not to understand how to utilize these functions but to comprehend the difference between rectangular and non-rectangular data. One may first consider grouping these items by section. For example, apples and bananas can be found in the produce section, whereas black beans and tomato sauce can be found in the canned goods. If we were to continue to group these items by section, we may arrive at a data set which looks something like this: groc &lt;- list(produce = data.frame(item = c(&#39;apples&#39;, &#39;bananas&#39;), price = c(3.99, 0.49)), condiments = data.frame(item = c(&#39;peanut_butter&#39;, &#39;mayonnaise&#39;), price = c(2.18, 3.89)), canned_goods = data.frame(item = c(&#39;black_beans&#39;, &#39;tomato_sauce&#39;), price = c(0.99, 0.69)), grains = data.frame(item = c(&#39;bread&#39;, &#39;pasta&#39;), price = c(2.99, 1.99)), dairy = data.frame(item = c(&#39;milk&#39;, &#39;butter&#39;), price = c(2.73, 2.57))) groc $produce item price 1 apples 3.99 2 bananas 0.49 $condiments item price 1 peanut_butter 2.18 2 mayonnaise 3.89 $canned_goods item price 1 black_beans 0.99 2 tomato_sauce 0.69 $grains item price 1 bread 2.99 2 pasta 1.99 $dairy item price 1 milk 2.73 2 butter 2.57 Here, we use lists and data frames to create a data set of our grocery list. This list can be traversed depending on what section of the store we find ourselves. For example, suppose we are in the produce section, and we need to recall what items to buy. We could utilize the following code to remind ourselves. groc$produce Is this grocery list an example of rectangular or non-rectangular data? Are there examples of rectangular data contained within the grocery list? How could we restructure the data to rectangularize the grocery list? As constructed, this grocery list is an example of non-rectangular data. As a whole, the grocery list is not shaped like a rectangle, but rather, consists of sets of rectangular data, where the sets are defined by the section of the store. Within a section of the store, the items and prices are given in rectangular form since every value is defined by a row and column. While non-rectangular data is often a useful return object for user-defined functions, they are often troublesome to work with. If a data set can be restructured or created in rectangular form, it should be. Rectangular data is especially important within the Tidyverse, a self-described ‘opinionated collection of R packages designed for data science’. All packages within the Tidyverse rely on the principle of tidy data, data structures where observations are given by rows and variables are given by columns. As defined, tidy data are rectangular, so as we embark on wrangling, visualizing, and modeling data in future chapters, it is important to ponder the nature of our data and whether it can be rectangularized. Let’s consider how we can rectangularize the grocery list. Instead of creating a list of named data frames, where the name represents the section of the store, let’s create a grocery list where each row represents an item and columns specify the section and price. Because the Tidyverse requires rectangular data, there are several functions which are handy for converting data structures to rectangular form. We could utilize one of these functions to rectangularize the data set. library(tidyverse, quietly = TRUE) groc_rec &lt;- groc %&gt;% bind_rows(., .id = &#39;section&#39;) groc_rec Or, we can simply create the grocery list in rectangular form to begin with. Any feedback for this section? Click here 2.1.1 Reading and Writing Rectangular Data Rectangular data are often stored locally using text files (.txt), comma separated value files (.csv), and Excel files (.xlsx). When data are written to these file types, they are easy to view across devices, without the need for R. Since most grocery store trips obviate the need for R, let’s consider how to write our grocery list to each of these file types. To write and read data to and from text files or comma separated value files, the readr package will come in handy, whereas the xlsx package will allow us to write and read to and from Excel files. To write data from R to a file, we will leverage commands beginning with write. # text file readr::write_delim(groc_rec, path = &#39;./data_raw/groceries-rectangular.txt&#39;) # csv file readr::write_csv(groc_rec, path = &#39;./data_raw/groceries-rectangular.csv&#39;) # Excel file xlsx::write.xlsx(groc_rec, file = &#39;./data_raw/groceries-rectangular.xlsx&#39;, row.names = FALSE) To read data from a file to R, we will leverage commands beginning with read. Before reading data into R, you will need to look at the file and file extension to better understand which function to use. # text file readr::read_delim(&#39;./data_raw/groceries-rectangular.txt&#39;, delim = &#39; &#39;) # csv file readr::read_csv(&#39;./data_raw/groceries-rectangular.csv&#39;) # Excel file xlsx::read.xlsx(&#39;./data_raw/groceries-rectangular.xlsx&#39;, sheetName = &#39;Sheet1&#39;) Reading files into R can sometimes be frustrating. Always look at the data to see if there are column headers and row names. Text files can have different delimiters, characters which separate values in a data set. The default delimiter for readr::write_delim() is a space, but other common text delimiters are tabs, colons, semi-colons, or vertical bars. Commas are so commonly used as a delimiter, it gets a function of its own. Always ensure that data from an Excel spreadsheet are rectangular. Lastly, the readr package will guess the data type of each column. Check these data types are correct using str(). Any feedback for this section? Click here 2.1.2 Reading and Writing Non-rectangular Data Writing non-rectangular data from R to your local machine is easy with the help of write_rds() from the readr package. While the origin of ‘RDS’ is unclear, some believe it stands for R data serialization. Nonetheless, RDS files store single R objects, regardless of the structure. This means that RDS files are a great choice for data which cannot be written to rectangular file formats such as text, csv, and Excel files. The sister function entitled read_rds() allows you to read any RDS file directly into your current R environment, assuming the file already exists. Similar to RDS files, there are also RData files which can store multiple R objects. These files can be written from R to your local machine using save() and read from your local machine to R using load(). We recommend avoiding RData files, and instead, storing multiple R objects in one named list which is then saved as an RDS file. When there is inevitably non-rectangular data that exist which you would like to load into R, you are in for a treat. The rest of this module can loosely be viewed as a guide to managing and curating data. We will leverage many tools to tackle this problem, but in the next two sections, we will address two specif, common instances of non-rectangular data: data from APIs and from scraped sources. Any feedback for this section? Click here "],
["apis-clean-and-curated.html", "2.2 APIs: Clean and Curated", " 2.2 APIs: Clean and Curated An application programming interface (API) is a set of functions and procedures which allows one computer program to interact with another. To simplify the concept remarkably, we will consider web-APIs where there is a server (computer waiting to provide data) and a client (computer making a request for data). The benefit of APIs is the result: clean and curated data from the host. The pre-processing needed to get the data in a workable form is entirely done on the server side. We, however, are responsible for making the request. Web-APIs often utilize JavaScript Object Notation (JSON), another example of non-rectangular data. We will utilize the httr and the jsonlite packages to retrieve the latest sports lines from Bovada, an online sportsbook. Before we start, we’ll need to download the httr and jsonlite packages and load them into our current environment. Furthermore, we will need to find the address of the server to which we will send the request. library(httr, quietly = TRUE) library(jsonlite, quietly = TRUE) bov_nfl_api &lt;- &quot;https://www.bovada.lv/services/sports/event/v2/events/A/description/football/nfl&quot; To ask for data through a web-API, we will need to make a GET request with the httr package’s GET() function. After making the request, we can read about the server’s response. bov_req &lt;- httr::GET(url = bov_nfl_api) bov_req Response [https://www.bovada.lv/services/sports/event/v2/events/A/description/football/nfl] Date: 2020-09-30 18:51 Status: 200 Content-Type: application/json;charset=utf-8 Size: 1.12 MB If the request was successful, then the status of the request will read 200. Otherwise, there was some error with your request. For a list of HTTP status codes and their respective definitions, follow this link. Since the response clarifies that the content is indeed driven by JavaScript, then we will utilize the jsonlite package to read the JSON structured data. A handy function we will use will be fromJSON() which converts a character vector containing data in JSON structure to native structures in R like lists. So, in order, we will Extract the content from the server’s response Convert the content to a character vector, maintaining the JSON structure Restructure the data into native R structures, using fromJSON(). content &lt;- bov_req$content content_char &lt;- rawToChar(content) bov_res &lt;- jsonlite::fromJSON(content_char) Of course, we could also create a function which takes the server’s response and converts the content to native R structures. We will want to code in a force stop if the response status is not 200. We will also want to require the httr and jsonlite packages which will automatically install the packages if a user calls the function without having the packages installed. convert_JSON &lt;- function(resp){ # call needed packages require(httr) require(jsonlite) # stop if the server returned an error httr::stop_for_status(resp) # return JSON content in native R structures return(jsonlite::fromJSON(rawToChar(resp$content))) } Finally, we can get the same output by simply calling the function. identical(convert_JSON(bov_req), bov_res) [1] TRUE Some web-APIs require additional information from the us as outlined in the documentation for the API. In this case, the user would need to provide additional query parameters in their GET request. Thankfully, this functionality is ingrained in the httr package’s GET() function. For more information on how to include query parameters, type ??GET into your R console. Any feedback for this section? Click here "],
["scraping-messy-and-mangled.html", "2.3 Scraping: Messy and Mangled", " 2.3 Scraping: Messy and Mangled If you are reading this textbook, at some point in your career, you are likely to want or need data which exists on the web. You have looked for downloadable sources and Google searched for an API, but alas, no luck. The last resort for importing data into R is web scraping. Web scraping is a technique for harvesting data which is portrayed on the web and exists in hypertext markup language (HTML), the language of web browser documents. 2.3.1 Scraping vs API The benefit of using an API are clean data. For example, we can traverse the result to find the latest NFL events. head(bov_res[[2]][[1]][,2]) [1] &quot;Denver Broncos @ New York Jets&quot; [2] &quot;Arizona Cardinals @ Carolina Panthers&quot; [3] &quot;Baltimore Ravens @ Washington Football Team&quot; [4] &quot;Cleveland Browns @ Dallas Cowboys&quot; [5] &quot;Indianapolis Colts @ Chicago Bears&quot; [6] &quot;Jacksonville Jaguars @ Cincinnati Bengals&quot; With more digging, we can find which teams are playing at home. head(bov_res[[2]][[1]][[16]]) [[1]] id name home 1 7736035-11904241 New York Jets TRUE 2 7736035-11904245 Denver Broncos FALSE [[2]] id name home 1 7736038-11904216 Carolina Panthers TRUE 2 7736038-11904247 Arizona Cardinals FALSE [[3]] id name home 1 7736049-16421961 Washington Football Team TRUE 2 7736049-11903831 Baltimore Ravens FALSE [[4]] id name home 1 7736033-11904234 Dallas Cowboys TRUE 2 7736033-11904219 Cleveland Browns FALSE [[5]] id name home 1 7736053-11903832 Chicago Bears TRUE 2 7736053-11904232 Indianapolis Colts FALSE [[6]] id name home 1 7736109-11904217 Cincinnati Bengals TRUE 2 7736109-11904220 Jacksonville Jaguars FALSE We can also find the current line of each of these games. Here, I have created a function called get_bovada_lines() which traverses this complicated (yet clean) JSON object using methods explored in Chapter 3 and combines the information together into a rectangular data set. head(get_bovada_lines(bov_json = bov_res)) While traversing these sometimes complicated lists may seem intimidating, with practice, working with data from an API will be made easier after discussing mapping functions in Chapter 3 which are useful for traversing complicated lists. Hopefully, after the scraping section, you will find working with APIs like a walk in the park compared to scraping data directly from the web. 2.3.2 Hypertext Markup Language (HTML) Web sites are written in hypertext markup language. All contents that are displayed on a web page are structured through HTML with the help of HTML elements. HTML elements consist of a tag and contents. The tag defines how the web browser should format and display the content. Aptly, the content is what should be displayed. For example, if we wished to format text as a paragraph within the web document, then we could use the paragraph tag, &lt;p&gt;, to indicate the beginning of a paragraph. After opening a tag, we then specify the content to display before closing the tag. A complete paragraph may read: &lt;p&gt; This is the paragraph you want to scrape. &lt;/p&gt; Attributes are optional parameters which provide additional information about the element in which the attribute is included. For example, within the paragraph tag, you can define a class attribute which formats the text in a specific way, such as bolding, coloring, or aligning the text. To extend our example, the element may read: &lt;p class = \"fancy\"&gt; This is the paragraph you want to scrape which has been formatted in a fancy script. &lt;/p&gt; The type of attribute, being class, is the attribute name, whereas the quantity assigned to the attribute, being fancy, is the attribute value. The general decomposition of an HTML element is characterized by the following figure: Figure 2.1: the lingo of an HTML element The class attribute is a flexible one. Many web developers use the class attribute to point to a class name in a style sheet or to access and manipulate elements with the specific class name with a JavaScript. For more information of the class attribute, see this link. For more information on cascading style sheets which are used to decorate HTML pages, see this link. Any feedback for this section? Click here 2.3.3 Selector Gadgets In the previous section, we introduced HTML elements. While all web pages are composed of HTML elements, the elements themselves can be structured in complicated ways. Elements are often nested inside one another or make use of elements in other documents. These complicated structures can make scraping data difficult. Thankfully, we can circumvent exploring these complicated structures with the help of selector gadgets. A selector gadget allows you to determine what css selector you need to extract the information desired from a webpage. These JavaScript bookmarklets allow you to determine where the information you desire belongs within the complicated structure of elements that makeup a webpage. To follow along in Chapter 3, you will need to download one of these gadgets from this link. If you use Google Chrome, you can download the bookmark extension directly from this link. If the selector gadget fails us, we can always view the structure of the elements directly by viewing the page source. This can be done by right-clicking on the webpage and selecting ‘View Page Source’. For Google Chrome, you can also use the keyboard shortcut ‘CTRL-U’. "],
["ScrapingInTheWild.html", "Chapter 3 Scraping in the Wild", " Chapter 3 Scraping in the Wild “You can have data without information, but you cannot have information without data.” — Daniel Keys Moran, Computer Scientist and Author In Chapter 2, we introduced the idea of rectangular data vs. non-rectangular data, providing examples for each and demonstrating the process of rectangularization. We outlined how to use a web-API before introducing the concept of web scraping by illustrating the language of the web: HTML. Since webpages can be complicated, scraping can be complicated. In this chapter, we will leverage the Selector Gadget and our knowledge of HTML elements to scrape data from various sources. It is our belief that the only way to teach web scraping is through examples. Each example will become slightly more difficult than the previous. "],
["LessonsLearnedFromScraping.html", "3.1 Lessons Learned from Scraping", " 3.1 Lessons Learned from Scraping Scraping is a necessary evil that requires patience. While some tasks may prove easy, you will quickly find others seem insurmountable. In this section, we will outline a few tips to help you become a web scraper. Brainstorm! Before jumping into your scraping project, ask yourself what data do I need and where can I find it? If you discover you need data from various sources, what is the unique identifier, the link which ties these data together? Taking the time to explore different websites can save you a vast amount of time in the long run. As a general rule, simplistic looking websites are generally easier to scrape and often contain the same information as more complicated websites with several bells and whistles. Start small! Sometimes a scraping task can feel daunting, but it is important to view your project as a war, splitting it up into small battles. If you are interested in the racial demographics of each of the United States, consider how you can first scrape this information for one state. In this process, don’t forget tip 1! Hyperlinks are your friend! They can lead to websites with more detailed information or serve as the unique identifier you need between different data sources. Sometimes you won’t even need to scrape the hyperlinks to navigate between webpages, making minor adjustments to the web address will sometimes do. Data is everywhere! Text color, font, or highlighting may serve as valuable data that you need. If these features exist on the webpage, then they exist within the HTML code which generated the document. Sometimes these features are well hidden or even inaccessible, leading to the last and final tip. Ready your search engine! Just like coding in R is an art, web developing is an art. When asking distinct developers to create the same website with the same functionality, the final result may be similar but the underlying HTML code could be drastically different. Why does this matter? You will run into an issue that hasn’t been addressed in this text. Thankfully, if you’ve run into an issue, someone else probably has too. We cannot recommend websites like Stack Overflow enough. "],
["scraping-nfl-data.html", "3.2 Scraping NFL Data", " 3.2 Scraping NFL Data In Chapter 2, we gathered some betting data pertaining to the NFL through a web-API. We may wish to supplement these betting data with data pertaining to NFL teams, players, or even playing conditions. As we progress through this sub-section, examples will become increasingly problematic or troublesome. The goal in this subsection is to introduce you to scraping by heeding the advice given in the Chapter 3.1. Following our own advice, let’s brainstorm. When you think of NFL data, you probably think of NFL.com or ESPN. After further digging, we will explore Pro Football Reference, a reliable archive for football statistics (with a reasonably simple webpage). This is an exhaustive source which boasts team statistics, player statistics, and playing conditions for various seasons. Let’s now start small by focusing on team statistics, but further, let’s limit our scope to the 2020 Denver Broncos. Notice, there are hyperlinks for each player documented in any of the categories, as well hyperlinks for each game’s boxscore where there is information about playing conditions and outcomes. Hence, we have a common thread between team statistics, players, and boxscores. If, for example, we chose to scrape team statistics from one website and player statistics from another website, we may have to worry about a unique identifier (being team) if the websites have different naming conventions. 3.2.1 HTML Tables: Team Statistics We’ll start with the team statistics for the 2020 Denver Broncos which can be found in a table entitled ‘Team Stats and Rankings’. We’ll need to figure in which element or node the table lives within the underlying HTML. To do this, we will utilize the CSS selector gadget. If we highlight over and click the table with the selector gadget, we will see that the desired table lives in an element called ‘#team_stats’. Figure 3.1: finding the team statistics element using the selector gadget Alternatively, we could view the page source and search for the table name. I’ve highlighted the information identified by the selector gadget with the cursor. Figure 3.2: finding the team statistics element using the page source While the selector gadget is always a great first option, it is not always reliable. There are instances when the selector gadget identifies a node that is hidden or inaccessible without JavaScript. In these situations, it is best view the page source directly for more guidance on how to proceed. Practice with both the selector gadget and the page source. Once we have found the name of the element containing the desired data, we can utilize the rvest package to scrape the table. The general process for scraping an HTML table is Read the HTML identified by the web address. Isolate the node containing the data we desire. Parse the HTML table. Take a look at the data to ensure the columns are appropriate labels. library(rvest) library(janitor) pfr_url &lt;- &quot;https://www.pro-football-reference.com&quot; broncos_url &lt;- str_c(pfr_url, &#39;/teams/den/2020.htm&#39;) broncos_url %&gt;% # read the HTML read_html(.) %&gt;% # isolate the node containing the HTML table html_node(., css = &#39;#team_conversions&#39;) %&gt;% # parse the html table html_table(.) %&gt;% # make the first row of the table column headers and clean up column names row_to_names(., row_number = 1) %&gt;% clean_names() While these data need cleaning up before they can be used in practice, we will defer these responsibilities to Chapter ??. In the next subsection, we will take a deeper dive into scraping HTML tables using information attained from attribute values, a common occurrence in web scraping. Take this time to scrape the ‘Team Conversions’ table on your own. Any feedback for this section? Click here 3.2.2 Commented and Hidden HTML Tables: Player Statistics Let’s transition to gathering player statistics, particularly for those players who have recorded statistics in the rushing or receiving category. These data are given in the table entitled ‘Rushing and Receiving’. Let’s use the selector gadget to identify the node containing the table and scrape the table according to the previous section. broncos_url %&gt;% # read the HTML read_html(.) %&gt;% # isolate the node containing the HTML table html_node(., css = &#39;#rushing_and_receiving&#39;) %&gt;% # parse the html table html_table(.) Error in UseMethod(\"html_table\"): no applicable method for 'html_table' applied to an object of class \"xml_missing\" We get an error stating that the node ‘#rushing_and_receiving’ does not contain an HTML table. In fact, there doesn’t appear to be anything in that node at all. broncos_url %&gt;% # read the HTML read_html(.) %&gt;% # isolate the node containing the HTML table html_node(., css = &#39;#rushing_and_receiving&#39;) {xml_missing} &lt;NA&gt; This, of course, is contrary to the selector gadget declaring a node ‘#rushing_and_receiving’ as the one containing the table we desire and directly see on the webpage. Generally when this happens, it means the node containing the information in the table has either been commented out or hidden. A node is commented out when it is contained in a comment tag: &lt;!-- --&gt;. For example, if we were to comment out the paragraph element in Chapter 2, it would look like this: &lt;!-- &lt;p class = \"fancy\"&gt; This is the paragraph you want to scrape which has been formatted in a fancy script. &lt;/p&gt; --&gt;. When a node is commented out, it exists in the HTML, but it cannot be accessed until we step into the comment tag. Otherwise said, we can’t bypass the comment node. If you would like to scrape information contained with a comment tag, the general strategy is Read the HTML identified by the web address. Isolate the node containing the comment tag. Inside the comment tag are the data we desire. Isolate the comment tag. Convert the content to HTML. Isolate the node containing the table. Parse the HTML table. Take a look at the data to ensure the columns are appropriate labels. broncos_url %&gt;% read_html() %&gt;% html_node(., css = &#39;#all_rushing_and_receiving&#39;) %&gt;% html_nodes(., xpath = &#39;comment()&#39;) %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(., &#39;table&#39;) %&gt;% html_table() %&gt;% set_names(., str_c(names(.), .[1,], sep = &#39;_&#39;)) %&gt;% clean_names() %&gt;% slice(., -1) 3.2.3 Attributes: Player Statistics by Game Suppose now that we wish to scrape the same player statistics, but rather than get the aggregated total, we want the players’ statistics by game. Within the Rushing and Receiving table scraped in the previous example, there are hyperlinks for each player. When you click on this hyperlink, you will find the player’s statistics for each game played. These are the data we desire. We want rushing and receiving statistics for each player on each team for a variety of years. Using the lessons learned in Chapter 3.1, we have brainstormed the problem and have a road map to the solution. We will first scrape the hyperlink attribute to get the web address associated with each player for a given team. Within each of these web addresses, we will scrape the game by game statistics before iterating over each team in the NFL. After this, we can repeat the procedure for a variety of years. This is, of course, a big job, so let’s scale down the problem to one player: Melvin Gordon III, running back. To do this, we will: Get the web addresses associated with each player in the Rushing and Receiving table, before isolating the address associated with Melvin Gordon’s rushing and receiving statistics. Read the HTML identified by the web address. Isolate the node containing the data we desire. Parse the HTML table. Take a look at the data to ensure the columns are appropriate labels. Let’s figure out where the hyperlinks exist within the HTML code. To do this, I will look at the page source and search for Melvin Gordon. We find that the web address corresponding to his game by game player statistics exist in the hidden HTML table that we scraped in the previous example. In particular, they are the href attribute to a node entitled a which is embedded within the hidden HTML table. Phew. Let’s consider how we can scrape these web addresses step by step: 1.1 Isolate the hidden HTML table, similar to the previous example. 1.2 Isolate nodes with tag a. 1.3 Extract the href attribute from these nodes. Steps 1.1 and 1.2 should be familiar from previous examples, but step 1.3 requires a new function within rvest called html_attr. Let’s see how to do this in R. Figure 3.3: finding the element containing the web address cooresponding to Melvin Gordon III game by game player statistics using the page source player_urls &lt;- broncos_url %&gt;% read_html() %&gt;% html_node(., css = &#39;#all_rushing_and_receiving&#39;) %&gt;% html_nodes(., xpath = &#39;comment()&#39;) %&gt;% html_text() %&gt;% read_html() %&gt;% html_nodes(., &#39;table&#39;) %&gt;% html_nodes(., &#39;a&#39;) %&gt;% html_attr(., &#39;href&#39;) %&gt;% str_c(pfr_url, .) player_urls [1] &quot;https://www.pro-football-reference.com/players/G/GordMe00.htm&quot; [2] &quot;https://www.pro-football-reference.com/players/L/LindPh00.htm&quot; [3] &quot;https://www.pro-football-reference.com/players/F/FreeRo00.htm&quot; [4] &quot;https://www.pro-football-reference.com/players/D/DrisJe00.htm&quot; [5] &quot;https://www.pro-football-reference.com/players/L/LockDr00.htm&quot; [6] &quot;https://www.pro-football-reference.com/players/H/HamlKJ00.htm&quot; [7] &quot;https://www.pro-football-reference.com/players/M/MartSa01.htm&quot; [8] &quot;https://www.pro-football-reference.com/players/F/FantNo00.htm&quot; [9] &quot;https://www.pro-football-reference.com/players/J/JeudJe00.htm&quot; [10] &quot;https://www.pro-football-reference.com/players/P/PatrTi00.htm&quot; [11] &quot;https://www.pro-football-reference.com/players/S/SuttCo00.htm&quot; [12] &quot;https://www.pro-football-reference.com/players/B/ButtJa00.htm&quot; [13] &quot;https://www.pro-football-reference.com/players/H/HamiDa01.htm&quot; [14] &quot;https://www.pro-football-reference.com/players/C/ClevTy00.htm&quot; [15] &quot;https://www.pro-football-reference.com/players/S/SpenDi00.htm&quot; [16] &quot;https://www.pro-football-reference.com/players/V/VannNi00.htm&quot; Since the web addresses are given as an extension to the Pro Football Reference homepage, we will need to concatenate the strings with str_c() before saving the result for future use. Now, lets continue to step two in our excursion to get Melvin Gordon’s game by game statistics. player_urls[1] %&gt;% read_html() %&gt;% html_node(., css = &#39;#all_stats&#39;) %&gt;% html_node(., &#39;table&#39;) %&gt;% html_table(., fill = TRUE) %&gt;% set_names(., str_c(names(.), .[1,], sep = &#39;_&#39;)) %&gt;% clean_names() %&gt;% slice(., -1) Any feedback for this section? Click here "],
["putting-it-all-together.html", "3.3 Putting It All Together", " 3.3 Putting It All Together Let’s consider how we can gather data regarding racial demographics across each state in the United States of America. Perusing Wikipedia, notice that each state has a table documenting the state’s ‘racial population breakdown’. Figure 3.4: part of brainstorming is exploring different websites which may have the data you desire If we were to go to Georgia, we would notice that the web address is similar to Alabama, but since Georgia is also a country, the web address specifies that you are currently on the webpage for the state rather than the country. This effectively rules out editing the web address to iterate over each state. If we broaden our search to ‘US States’, we find a Wikipedia page with a list of each state. Each state has a hyperlink which takes us to the state’s webpage with the demographic information we desire. Remember: hyperlinks are our friend! If they exist, we can scrape them, iterating over the web addresses to get data from each state. Let’s use what we just learned to attain the web addresses for each state. We’ll start with the web address containing a list of every state. wiki_url &lt;- &quot;https://www.wikipedia.org&quot; states_url &lt;- str_c(wiki_url, &#39;/wiki/U.S._state#States_of_the_United_States&#39;) Let’s use the page source to find where the hyperlinks exist in the HTML code. To do this, I am going to search for the phrase ‘The 50 U.S. states, in alphabetical order, along with each state’s flag’ in the page source since the list of states is just below this phrase. Figure 3.5: finding the element containing the list of states with their cooresponding web addresses using the page source We find that the list of states, among other information, exists in an element tagged div with the attribute class set to an attribute value of plainlist. Within this node, the list of states exists in an element tagged ul and each state exists in an element tagged li. Within each state, the web address corresponding to that state’s individual Wikipedia article is given in the href attribute to an element tagged a. Let’s use this information to extract the web addresses corresponding to each state. We may also want to extract the state name which is the content of the element tagged a. node_w_states &lt;- states_url %&gt;% read_html() %&gt;% html_nodes(xpath = &quot;//div[@class=&#39;plainlist&#39;]&quot;) %&gt;% html_nodes(&#39;ul&#39;) %&gt;% html_nodes(&#39;li&#39;) %&gt;% html_nodes(&#39;a&#39;) ind_states &lt;- tibble(state = node_w_states %&gt;% html_text(), url = node_w_states %&gt;% html_attr(., &#39;href&#39;) %&gt;% str_c(wiki_url, .)) ind_states Let’s consider how we would scrape the racial demographics. Use either the selector gadget or the page source to identify where the desired table exists in the HTML code. ind_states$url[1] %&gt;% read_html() %&gt;% html_nodes(., xpath = &quot;//table[@class=&#39;wikitable sortable collapsible&#39;]&quot;) %&gt;% html_table() %&gt;% as.data.frame() In the next chapter, we will learn the tools to scrape the racial demographics for each state using the vector of web addresses we attained through this process as well as cleaning up these data according to tidy principles. Any feedback for this section? Click here "]
]
